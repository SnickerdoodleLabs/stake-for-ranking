\section{Implementation}
\label{section:Implementation}
%------------------------------------------------------------------------------------------
\subsection{Architecture Abstract}
\label{section:Architecture}

This section will discuss the implementation and design decisions of the Protocol. The Protocol has three primary 
components: a data wallet implementation, an on-chain data control plane, and aggregation service providers.

The data wallet is a software client that implements functionality which enables end-users to collect, index, and store their data as well as participate 
in the decentralized data network, see section \ref{section:DataWallet}. Organizations (consumers of data insights) will be able 
to query populations of data wallets through an on-chain control plane that adheres to a publish-subscribe (pub-sub) pattern, see section \ref{section:OnChain}. 

At the core of the control plane is an upgradable contract factory which produces independent instances of an EIP-721 compatible consent registry. 
Consent tokens claimed from these consent contracts are non-transferable but can be burned by the recipient. Claiming a consent token denotes a data 
wallet user's consent to participate in network queries in return for rewards (which may or may not be web3 digital assets). 

Data wallets receive queries via EVM events emitted from consent registry contracts they have claimed tokens in. These events contain metadata encoding
instructions (written in SDQL) to run computations on the data stored in the data wallet to produce insights. Once a data wallet has produced
the requested insight, it performs a digital "handshake" with the aggregation service provider specified in the query metadata such that the 
data wallet owner receives a reward while the requesting organization receives the insight, see figure \ref{fig:OnChainOffChain}. 

User consent and data flow is thus orchestrated in a distributed manner by the Protocol. It is also worth highlighting that while 
Snickerdoodle Labs will develop service infrastructure for the protocol (such as producing a data wallet and a SaaS product offering 
for enterprise participation in the protocol), the protocol itself is, however, permissionless and open so that anyone could implement a 
data wallet client or act as an aggregation service provider if the specifications of the protocol is adhered to. 
%----------------------------------------------------------------------------------------------------------------------------------------
\subsection{On-Chain Components}
\label{section:OnChain}

\subsubsection{Consent Contract Factory}
\label{section:ConsentFactory}

\input{ConsentContractPubSubTikz}
The on-chain components of the Protocol function as a decentralized, permissionless data control plane. It specifically implements a publish-subscribe pattern
in which organizations publish new instances of an EIP-721 compatible consent registry (see \ref{section:ConsentContract}), and end-users 
subscribe to the registries by claiming a non-transferrable consent token. The publishing action is performed via the Protocol's upgradable 
consent contract factory, see figure \ref{fig:ConsentFactory}. The factory contract will be the entrypoint to the Protocol for new insight 
consumers, since a consent registry is required to communicate with the network of data wallets. 

\input{ContractUpgradePatternTikz}
The consent contract factory exists as a utility for insight consumers to create new consent registries. The factory is implemented with an 
upgradable beacon pattern, see figure \ref{fig:UpgradePattern} to enable gas-efficient deployments and to allow for seamless extensions of functionality
via DAO proposals (see sections \ref{section:ImplementationDAO} and \ref{section:Governance}) . 

\paragraph{Factory Pattern}
The factory pattern defines a smart contract which is responsible for creating other contracts. The Protocol uses the factory 
pattern to simplify the deployment of new consent registries and to give new insight consumers a single point of entry into the data network. 

\paragraph{Upgradable Beacon Pattern}
\label{section:BeaconPattern}

Consent registries are deployed as proxy contract instances that reference an upgradable beacon contract to obtain the correct address to delegate
function calls, see figure \ref{fig:UpgradePattern}. This upgrade pattern compliments the factory pattern by allowing for very gas-efficient deployments 
of new proxy instances. Proxy contracts only store storage variables and a pointer to their designated upgradable beacon contract. The upgradable 
beacon contract points to an implementation contract. The implementation contract contains all function implementations as well as the storage
variable declarations that proxy contracts copy. 

A Protocol upgrade to the consent registry functionality requires that a new implementation contract first be deployed to the blockchain. Then a 
DAO proposal must be initiated to point the upgradeable beacon to the new implementation address. All previously deployed and newly created proxy 
consent contracts inherit the functionality (and any new storage variables) defined in the new contract.

The upgradeability pattern is based on EIP-1967 and is implemented through OpenZeppelin libraries.

\subsubsection{Consent Registries}
\label{section:ConsentContract}

\input{OnChainOffChainInteractionTikz}
Consent registry contracts are the primary on-chain mechanism by which insight consumers interact with with data wallet end-users. Consent 
registries allow organizations to create data pools by serving as an on-chain data structure that holds metadata regarding the conditions 
under which data is to be collected and used as well as a cryptographically verifiable list of externally owned accounts (EOAs) that have
have given consent to participate in the data pool. 

Consent registries expose an EIP-721 compatible interface. This is for developer integration convenience as it allows consent registries to 
be readable by most existing NFT indexing services (such as Snowtrace). Consent is denoted by ownership of a non-transferrable consent NFT 
(non-fungible token) which can be burned by the NFT owner at any time. 

\paragraph{Request-for-Data Events}
\label{section:RequestForData}

After an organization has published a consent registry via the Consent Contract Factory (see \ref{section:ConsentFactory}), the organization can emit
EVM events by calling a special function, $requestForData$, which takes a content identifier (CID) as its only input. This CID is used to 
retrieve the request specifications from a suitable content addressable network (like IPFS) that the data wallets will process (see section \ref{section:AquisitionControlFlow}). Data wallets can detect 
past $requestForData$ events by constructing EVM query filters and requesting all EVM logs that match those filters. Thus consent registries offer a 
tamper-resistant communication layer between organizations and participants in their data pools. See figure \ref{fig:OnChainOffChain}

Consent registries also specify a $queryHorizon$ which inform data wallets of the oldest block number to search for these events. This variable is 
first initialized to the block number of the proxy contract deployment from the contract factory and can be updated by an EOA with the $DEFAULT\_ADMIN\_ROLE$
to a later blocknumber (but cannot be changed to an earlier block number). Setting a reasonable value for $queryHorizon$ is important since many RPC providers
only allow for query filters to search a limited history of the blockchain.

\paragraph{User Data Permissioning}
\label{section:UserPermissions}

A $requestForData$ event can indicate that it requires access to multiple attributes indexed by the user's data wallet client, such as country of origin, age,
on-chain contracts they have interacted with using their linked EOA asset account. However, users can set granular permissions regarding their indexed data 
attributes. 

Every consent token issued from a consent registry has an associated set of binary $agreementFlags$. There are 256 flags in total (the size of an EVM word) 
though not all flags will be assigned to specific attributes at Protocol launch, leaving room for customization. Only the owner of a consent token can 
update the granular permissions denoted by the token's $agreementFlag$s. The availability of granular consent data on-chain allows organizations to better 
understand what kinds of insights they will be able to obtain from their data pool before calling $requestForData$. 

\paragraph{Consent Invitations}
\label{section:ConsentInvitations}

Consent registry metadata is used for the decentralized, trustless, and tamper-resistant triggering of user flows that should be presented to the data wallet 
end user in a format appropriate for the data wallet client environment. In a web browser setting, the data wallet detects the current active URL, and via DNS 
over HTTPS (DoH) queries the TXT records associated with the apex domain. If the TXT record contains a reference to a Protocol consent 
contract address, the data wallet then fetches the URLs registered in the consent registry $domains$ metadata storage variable and cross-references the domains 
listed from the contract to the current URL. If the data wallet detects that the current URL is included in the domains listed in the 
consent contract, the data wallet will inject a user flow into the browser DOM. The content of the user-flow is fetched from the URL specified by the consent 
registry $baseURI$ parameter. 
\input{Web3PopupTikz}
The Web3 popup protocol can be extended to other environments including mobile browsing environments, VR experiences, gaming consoles, etc as indicated by 
figure \ref{fig:PopupProtocol}. The user flows presented by these tamper-resistant popups serve as the on-boarding mechanism for end-user's to opt into a data pool.

\paragraph{Opt-In methods and Meta-Transactions}
\label{section:OptInMethods}

The consent registries have two modes by which users can join a data pool: open-access and invite-only. Consent registries in which open-access is 
enabled ($openOptInDisabled$ is $false$), any EOA is allowed to claim a consent token by calling the $optIn$ method and paying the associated gas
fees. 

Registries where $openOptInDisabled$ is $true$ are invitation only and require a signature for an EOA with the $SIGNER\_ROLE$ in order to join 
the data pool associated with a consent registry. There are two available methods for invitation-only user opt-in: $restrictedOptIn$ and $anonymousRestrictedOptIn$. 

The former method
requires that the recipient EOA be known in advance by the $SIGNER\_ROLE$ in order to construct the appropriate signature. The receiving EOA then 
becomes the only account that can call $restrictedOptIn$ with that signature. If the recipient EOA address is not known ahead of time, the $SIGNER\_ROLE$
can construct a signature for use with the $anonymousRestrictedOptIn$ method in which any EOA can submit the signature to that method in order to opt-in. Once a signature that was constructed for use with either invite only method is used, it cannot be used a second time. 

Support for both open and invitation-only opt-in flows makes the Protocol flexible to a variety of use-cases. However, end users are often hesitant to 
spent their own cryptocurrency in order to pay for transaction fees associated with a decentralized application or simply do not have the tokens 
necessary to do so. Additionally, requiring the user to spend their own assets to participate in the Protocol intoduces significant user friction and
adoption hurdles. Consent registries implement EIP-2772 compatible metatransaction capabilities to circumvent this issue. 

Metatransactions allow someone else to pay for a user's gas fees. All opt-in methods  implement support for metatransactions. This will offer the 
flexibility to have users pay for their own gas or have the businesses pay.  

\subsubsection{Identity Crumbs}
\label{section:Crumbs}

The Protocol introduces a special EIP-721 compatible registry, called the Crumbs Contract, to facilitate data wallet synchronization when an end user 
installs the client on a new device. When a user links a new EOA to their data wallet identity, the EOA encrypts their data wallet identity EOA (see section
\ref{section:UserIdentityGeneration}) and stores the encrypted data in the token URI of an entry in the Crumbs contract. 

During a new client installation, the end user must simply link any EOA that has previously been linked to their data wallet identity in order for the data 
wallet to synchronize from their previously saved state that has been stored in the decentralized persistence layer of the data wallet network. The data wallet
checks if the account being linked owns a token in the Crumbs contract; if so, it reads the encrypted content of the token URI, decrypts the information, and loads 
the public-private key pair into memory. 

\subsubsection{EIP-20 Token}
\label{section:Token}
The Protocol includes a fungible utility token adhering to the EIP-20 standard. This token will be used for paying various fees required to leverage
the Protocol (like publishing a new consent registry) and will also be used for voting in the Protocol DAO. The associated voting mechanism that 
accompanies the possession of a token can be delegated to a different address without relinquishing ownership of the token

\subsubsection{Decentralized Autonomous Organization}
\label{section:ImplementationDAO}

\input{ProposalLifecycleTikz}
The Protocol will include a decentralized autonomous organization (DAO) implementation as part of its on-chain components. The DAO will be responsible for proposing and executing upgrades to 
the Protocol. The particular pattern used by the Protocol DAO is based on Curve Finance's DAO and will be implemented with OpenZeppelin libraries. 

Token holders (see section \ref{section:Token}), are responsible for the creation and execution of DAO proposals. At mainnet launch it is anticipated that one 
token will render one vote, though this too could be modified via a DAO proposal. Token holders will have to reach a pre-specified quorum of voting power in 
order to successfully create a proposal in the DAO task queue. 

Proposals will be subject to a delay of at least one block before voting begins as well as before a queued proposed can be executed 
in order to prevent flash loan attacks. Voters who initiate a proposal and subsequently relinquish their voting power by either selling their tokens or have their voting power revoked may have their proposal canceled if their remaining voting power is below the quorum threshold. The lifecycle of a DAO proposal is outlined in figure \ref{fig:ProposalLifecycle}.
%----------------------------------------------------------------------------------------------------------------------------------------
\subsection{Off-Chain Components}
\label{section:OffChain}

\subsubsection{Data Wallet}
\label{section:DataWallet}

\input{DataWalletStructureTikz}

The data wallet is the primary client interface for end users to interact with the Protocol. It enables data ownership by facilitating user control 
and consent to the collection, storage, and usage of their data. The data wallet should provide the following functionality (as indicated 
by figure \ref{fig:DataWalletStructure}):

\begin{itemize}
  \item Ingestion and indexing of user data from the data wallet client deployment environment
  \item Identity and verifiable credential generation/management
  \item Query/Reward discovery and management
  \item A query engine for individualized data mining, insight processing and delivery
  \item A consent management interface and granular access control
  \item Secure storage of the data
\end{itemize}

To the user, the data wallet operates in a conceptually similar way to a conventional cryptocurrency wallet, but with a wider scope. 
Instead of key and account management of a blockchain account, the primary purpose of a data wallet is to manage the storage, collection, and sharing of insights derived 
from user data. Data wallet functionality is form-factor agnostic (see section \ref{section:FormFactor}). However, browser extension and 
mobile applications are anticipated to be the primary channels for use.

\paragraph{User Data Acquisition and Insight Control Flow}
\label{section:AquisitionControlFlow}

\input{InsightControlFLowTikz}

The acquisition of insights from the data network begins at the consent contract factory (see section \ref{section:ConsentFactory}) where an organization 
must first publish a consent registry. Data wallets belonging to end users who have claimed a consent token via an invitation flow (section \ref{section:ConsentInvitations})
detect $requestForData$ events from the associated consent registry. The event will reveal a CID which resolves to query definition file (section \ref{section:RequestForData}). 
The query execution layer of the data wallet will parse the query definition, construct the associated abstract syntax tree (AST), and apply the logic to the
data wallet persistence layer in a manner consistent with the conditions given by the user's on-chain permission settings (section \ref{section:UserPermissions}). This
control flow is outlined in figure \ref{fig:InsiteControlFlow}.

\paragraph{User Identity Generation via Key Ratchets}
\label{section:UserIdentityGeneration}

A data wallet should allow users to index transaction history and asset owership from multiple EOAs or smart wallets. However, if they were to use these addresses directly in the participation of 
the Protocol for consent token ownership it would readily allow for chain analysis of user behavior and compromise user data privacy. Therefor, a data wallet
implementation should provide key ratchet utilities to allow for the deterministic generation of new EOAs that cannot be linked back to the generating EOAs which have the dedicated purpose of holding consent tokens. 

A ratchet is a simple machine that only allows unidirectional state increments and prevents backward traversal of the state path. Likewise, a cryptographic
key ratchet is an algorithm that allows for the deterministic generation of new public-private key pairs in a manner that precludes the feasibility of determining
what public-private key pairs were used in previous iterations. Cryptographic ratchet algorithms are widely used in consumer-facing private messaging applications

\begin{algorithm}
\caption{Key Ratchet Proto-algorithm}
\label{alg:KeyRatchet}
    \begin{algorithmic}
        \Require EOA with message signing utility, Seed Message
        \Ensure New EOA with no prior transaction history
        \State Seed Message Signature $\gets$ EOA.signMessage(Seed Message)
        \State new EOA $\gets$ pbkdf2sync(Seed Message Signature, EOA public address, 100000, 32, sha256)
        \State \textbf{return} new EOA
    \end{algorithmic}
\end{algorithm}

Procedure \ref{alg:KeyRatchet} outlines a simple technique, leveraging the Password-Based Key Derivation Function 2 (pbkdf2sync), to generate a new public-private key pair from the message signing utility exposed from most consumer crypto-wallets. This 
algorithm, used in conjunction with the Crumbs contract, described in section \ref{section:Crumbs}, can be used to enhance user data privacy by preventing the cross-referencing of linked EOAs on-chain while at the same time offering an improved user
experience. 

Specifically, by using the key ratchet algorithm to derive a dedicated in-memory key pair for consent, the data wallet form factor can sign metatransactions (see section \ref{section:OptInMethods}) without multiple prompts from various wallet applications. Additionally, using derived EOAs for consent provides an additional layer of security for end user's, keeping their valuable asset-holding EOAs separate from those
used for participating in various data pools. 

\paragraph{Storage}
\label{section:storage}

Secure storage of data is crucial to allowing end users to own their data. The initial data wallet implementation produced by 
Snickerdoodle Labs exposes a modular storage interface capable of integrating with various storage provider technologies, 
such as Google Storage, Amazon S3, or decentralized options like the Ceramic Network.

It is also important to call out the wallet's storage of public-private key pairs. A data wallet should only store public keys 
and digital signatures associated with accounts linked to a user's data wallet, not private keys. 
Instead, we integrate with existing wallets (e.g., MetaMask) for key management.

\paragraph{Data Ingestion}
\label{section:DataIngestion}
The data wallet will also help users own their own data by allowing them to collect the data that they generate. This individualized data mining 
provides highly accurate data that the protocol allows users to easily monetize. The data the wallet collects has three important properties: 
explicit/implicit, first/third party, and authenticated/unauthenticated. %Not sure if there's a better term

% should be included but I'm not sure where or how
An important feature to highlight is that multiple addresses can be linked together in the same data wallet, including wallets for separate chains. 

% Not sure if this should be a paragraph or points
\paragraph{Explicit/Implicit}
The data wallet will either collect data through explicit or implicit means. Explicit data is data directly provided by the user, such as their name, 
age, or wallet address. In the initial version of the data wallet, the user will manually input this data; however, in the future, the data wallet 
could feature in-browser event capturing to passively collect this data (e.g., collecting information about what websites a user has visited).


Implicit data is data generated by user action. For example, using a user's wallet address to learn that they swapped specific tokens or have played a web3 game. 


\paragraph{First/Third Party}
Data collected can come from different sources. Specifically, first-party data comes directly from the user, and third-party data comes from 
someone other than the user. For example, if the user directly inputs their name, their name would be considered first-party data; if the 
user imports their name from the DMV, that would be third-party data.


\paragraph{Authenticated/Unauthenticated}
Data can be authenticated if its origin and validity can be verified through cryptographic means and unauthenticated if no such mechanism exists. For example, 
a wallet address can be authenticated via a signed message signature verification. Third-party data can be authenticated if it has a known 
credential authority (this is the fundamental operating principal of certificate authorities like Digicert). 

\paragraph{Localized Processing} % listens to events and processes queries

The data wallet is a local application that stores the owner's data securely and processes computations locally. By collecting and 
securely storing user data locally, the data wallet guarantees data ownership to the user by never sharing it. Because computations 
are running locally, the owner ensures that only analysis they've given consent to can run on their data. Businesses also benefit 
from this model as they can leverage data-driven insights without worrying about the liability or infrastructure of storing raw, 
personal data. While the initial version of localized processing will be more limited, there is a myriad of ways we can modify this 
approach to add additional data safety and features (see section \ref{section:Future}). 

The wallet will learn what computations to run by listening to on-chain events. These events will be emitted on the Snickerdoodle Avalanche 
subnet and specify SDQL queries that include the computation instructions. If those data request events are from consent contracts that the 
user has opted into, the data wallet will process the data following the rules in the associated SDQL query. After the localized processing 
has been completed, the data wallet will send the insights to the specified endpoint, and the data owner will collect any rewards.


\subsubsection{Protocol Query Language} % maybe move to contracts 
\label{section:SDQL}

The Protocol specifies a simple query language which will allow businesses to broadcast conditional data requests to consent registry cohorts in a transparent and interpretable fashion. 
It will be structured as a simple JSON file containing information on the eligibility requirements, rewards, data to be collected, what processing 
to perform, and where to send processed data (see figure TODO). The allowed functions and syntax of the language will be enforced by the 
Snickerdoodle DAO and determined by token holders, which will allow for its continued development and the enforcement of user privacy. SDQL 
queries must be stored on a content-addressable distributed network, such as IPFS, to ensure tamper resistance. 

TODO FIGURE SHOWING QUERY + flesh out sections

\paragraph{Rewards}

An essential part of the protocol is allowing individuals to get value from their rewards. The data wallet will facilitate this by allowing users 
to gain rewards for sharing their data. First, the data wallet helps people find rewards that are being offered. If a person goes to a website 
that's enabled the Snickerdoodle Protocol, they will see a pop-up and can connect their data wallet to check out possible rewards. Additionally, 
the data wallet will have a section showing available rewards.


Once the user has given consent to share data, the data wallet will also help the user manage their rewards. When they receive a query, they will 
be able to visualize the rewards offered in that query. The owner will also be able to set which addresses and chains they want to receive rewards. 
Additionally, the owner will be able to see the history of the rewards they've received and the data exchanged. 

\subsubsection{Aggregation Services} % name for role?

Data ingestion and aggregation is the final stage of the insight aggregation process that allows subscribers to access the insights generated at the data wallet layer. 
This section will detail how the subscriber can see processed data from the user's data wallet. Any entity can assume the role of data ingestion service 
as long as they follow the protocol and are accepted by the DAO. Snickerdoodle Labs will offer the first data ingestion provider, which will be tied to 
a SaaS product. 

\paragraph{Insight Ingestion}
The final step of the protocol is for data wallets to send insights from their queries to an endpoint. This endpoint is the ingestion provider and is 
specified in the SDQL query. Ingestion is an integral part of the protocol as it allows businesses to see the insights they have paid for.

Once the ingestion provider receives the insight, it must store the insights events and manage access. They can also provide any additional services 
they want on top of this, such as providing a dashboard to view insights. There is no way to mathematically guarantee any ingestion provider is behaving 
honestly or doesn't have any bugs. The lack of a strong guarantee means that actors in the protocol have to put some trust in ingestion providers. The 
Snickerdoodle Protocol reduces this problem by only allowing ingestion services that the DAO has approved. Any provider has to be trustworthy enough for 
the DAO to vote them in; if a provider is revealed to be a bad or incompetent actor, they can be voted out. For a deeper discussion of data safety 
concerns with ingestion providers, see section \ref{section:IngestionDataSafety}.

\paragraph{Data Safety}
\label{section:IngestionDataSafety}

The initial version of the protocol requires trust in the Insight Platform to maintain data safety. As discussed above, there are no guarantees that the 
ingestion provider acts honestly. The ingestion provider can see, delete, and sell any insights they receive. Additionally, because the initial version 
of the wallet only implements simple anonymization techniques, ingestion providers could identify who shared the insight and reconstruct the raw data. 


To reduce these privacy risks, the initial version of the protocol will only allow queries that don't reveal PII, and future versions will add anonymization 
techniques to insight computations (see section \ref{section:Future}). To reduce the risk of malicious ingestion services, the Snickerdoodle DAO will maintain 
an allowlist of trusted actors. Additionally, Snickerdoodle Labs will be providing an ingestion service. The Snickerdoodle Ingisht Service is a SaaS product 
that will enforce data safety and has a massive financial and legal incentive to behave honestly.

\paragraph{SDL Insight Platform}
\label{section:InsightService}

The Snickerdoodle Insight Platform will be our SaaS product offering for interaction with the Snickerdoodle Protocol. The Insight Platform will provide an 
ingestion service and help manage interaction with the rest of the protocol. This includes support for creating and managing queries, consent contracts, 
rewards, and website integrations. The Insight Platform will also provide an analytics dashboard to help businesses see their insights.