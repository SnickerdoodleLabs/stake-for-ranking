\section{Implementation}
\label{section:Implementation}
%------------------------------------------------------------------------------------------
\subsection{Architecture}
\label{section:Architecture}

This section will discuss the implementation and design decisions of the Protocol. The Protocol has three primary 
components: a data wallet implementation, an on-chain data control plane, and aggregation service providers.

The data wallet is a software client that implements functionality which enables end-users to collect, index, and store their data as well as participate 
in the decentralized data network, see section \ref{section:DataWallet}. Organizations (consumers of data insights) will be able 
to query populations of data wallets through an on-chain control plane that adheres to a publish-subscribe (pub-sub) pattern, see section \ref{section:OnChain}. 

At the core of the control plane is an upgradable contract factory which produces independent instances of an EIP-721 compatible consent registry. 
Consent tokens claimed from these consent contracts are non-transferable but can be burned by the recipient. Claiming a consent token denotes a data 
wallet user's consent to participate in network queries in return for rewards (which may or may not be web3 digital assets). 

Data wallets receive queries via EVM events emitted from consent registry contracts they have claimed tokens in. These events contain metadata encoding
instructions (written in SDQL) to run computations on the data stored in the data wallet to produce insights. Once a data wallet has produced
the requested insight, it performs a digital "handshake" with the aggregation service provider specified in the query metadata such that the 
data wallet owner receives a reward while the requesting organization receives the insight, see figure \ref{fig:OnChainOffChain}. 

User consent and data flow is thus orchestrated in a distributed manner by the Protocol. It is also worth highlighting that while 
Snickerdoodle Labs will develop service infrastructure for the protocol (such as producing a data wallet and a SaaS product offering 
for enterprise participation in the protocol), the protocol itself is, however, permissionless and open so that anyone could implement a 
data wallet client or act as an aggregation service provider if the specifications of the protocol is adhered to. 
%----------------------------------------------------------------------------------------------------------------------------------------
\subsection{On-Chain Components}
\label{section:OnChain}

\subsubsection{Consent Contract Factory}
\label{section:ConsentFactory}

\input{ConsentContractPubSubTikz}
The on-chain components of the Protocol functions as a decentralized, permissionless data control plane. It specifically implements a publish-subscribe pattern
in which organizations publish new instances of an EIP-721 compatible consent registry (see \ref{section:ConsentContract}), and end-users 
subscribe to the registries by claiming a non-transferrable consent token. The publishing action is performed via the Protocol's upgradable 
consent contract factory, see figure \ref{fig:ConsentFactory}. The factory contract will be the entrypoint to the Protocol for new insight 
consumers, since a consent registry is required to communicate with the network of data wallets. 

\input{ContractUpgradePatternTikz}
The consent contract factory exists as a utility for insight consumers to create new consent registries. The factory is implemented with an 
upgradable beacon pattern, see figure \ref{fig:UpgradePattern} to enable gas-efficient deployments and to allow for seamless extensions of functionality
via DAO proposals. 

\paragraph{Factory Pattern}
The factory pattern defines a smart contract which is responsible for creating other contracts. The Protocol uses the factory 
pattern to simplify the deployment of new consent registries and to give new insight consumers a single point of entry into the data network. 

\paragraph{Upgradable Beacon Pattern}
\label{section:BeaconPattern}

Consent registries are deployed as proxy contract instances that reference an upgradable beacon contract to obtain the correct address to delegate
function calls, see \ref{fig:UpgradePattern}. This upgrade pattern compliments the factory pattern by allowing for very gas-efficient deployments 
of new proxy instances. Proxy contracts only store storage variables and a pointer to their designated upgradable beacon contract. The upgradable 
beacon contract points to an implementation contract. The implementation contract contains all functions implementations as well as the storage
variable declarations that proxy contracts copy. 

A Protocol upgrade to the consent registry functionality requires that a new implementation contract first be deployed to the blockchain. Then a 
DAO proposal must be initiated to point the upgradeable beacon to the new implementation address. All previously deployed and newly created proxy 
consent contracts inherit the functionality (and any new storage variables) defined in the new contract.

The upgradeability pattern is based on EIP-1967 and is implemented through OpenZeppelin libraries.

\subsubsection{Consent Registries}
\label{section:ConsentContract}

\input{OnChainOffChainInteractionTikz}
Consent registry contracts are the primary on-chain mechanism by which insight consumers interact with with data wallet end-users. Consent 
registries allow organizations to create data pools by serving as an on-chain data structure that holds metadata regarding the conditions 
under which data is to be collected and used as well as a cryptographically verifiable list of externally owned accounts (EOAs) that have
have given consent to participate in the data pool. 

Consent registries expose an EIP-721 compatible interface. This is for developer integration convenience as it allows consent registries to 
be readable by most existing NFT indexing services (such as Snowtrace). Consent is denoted by ownership of a non-transferrable consent NFT 
(non-fungible token) which can be burned by the owner at any time. 

\paragraph{Request for Data Events}
\label{section:RequestForData}


After an organization has published a consent registry via the Consent Contract Factory (see \ref{section:ConsentFactory}), the organization can emit
EVM events by calling a special function, $requestForData$, which takes a content identifier (CID) as its only input. This CID is used to 
retrieve the request specifications from a suitable content addressable network (like IPFS) that the data wallets will process. Data wallets can detect 
past $requestForData$ events by constructing EVM query filters and requesting all EVM logs that match those filters. Thus consent registries offer a 
tamper-resistant communication layer between organizations and participants in their data pools. See figure \ref{fig:OnChainOffChain}

Consent registries also specify a $queryHorizon$ which inform data wallets of the oldest block number to search for these events. This variable is 
first initialized to the block number that the proxy contract was deployed from the contract factory and can be updated by an EOA with the $DEFAULT\_ADMIN\_ROLE$
to a later blocknumber (but cannot be changed to an earlier block number). Setting a reasonable value for $queryHorizon$ is important since many RPC providers
only allow for query filters to search a limited history of the chain.

\paragraph{User Data Permissioning}
\label{section:UserPermissions}

A $requestForData$ event indicate that it requires access to multiple attributes indexed by the user's data wallet client, such as country of origin, age,
on-chain contracts they have interacted with using their linked EOA asset account. However, users can set granular permissions regarding their indexed data 
attributes. 

Every consent token issued from a consent registry has an associated set of binary $aggreementFlags$. There are 256 flags in total (the size of an EVM word) 
though not all flags will be assigned to specific attributes at Protocol launch, leaving room for customization. Only the owner of a consent token can 
update the granular permissions denoted by the token's $agreementFlag$s. The availability of granular consent data on-chain allows organizations to better 
understand what kinds of insights they will be able to obtain from their data pool before calling $requestForData$. 

\paragraph{Consent Invitations}
\label{section:ConsentInvitations}


Consent registry metadata is used for the decentralized, trustless, and tamper-resistant triggering of user flows that should be presented to the data wallet 
end user in a format appropriate for the data wallet client environment. In a web browser setting, the data wallet detects the current active URL, and via DNS 
over HTTPS (DoS) queries the TXT records associated with the apex domain. If the TXT record contains a reference to a Protocol consent 
contract address, the data wallet then fetches the URLs registered in the consent registry $domains$ metadata storage variable and cross-references the domains 
listed from the contract to the current URL. If the data wallet detects that the current URL is included in the domains listed in the 
consent contract, the data wallet will inject a user flow into the browser DOM. The content of the user-flow is fetched from the URL specified by the consent 
registry $baseURI$ parameter. 
\input{Web3PopupTikz}
The Web3 popup protocol can be extended to other environments including mobile browsing environments, VR experiences, gaming consoles, etc as indicated by 
figure \ref{fig:PopupProtocol}. The user flows presented by these tamper-resistant popups serve as the on-boarding mechanism for end-user's to opt into a data pool.

\paragraph{Opt-In methods and Meta-Transactions}
\label{section:OptInMethods}

The consent registries have two modes by which users can join a data pool: open-access and invite-only. Consent registries in which open-access is 
enabled ($openOptInDisabled$ is $false$), any EOA is allowed to claim a consent token by calling the $optIn$ method and paying the associated gas
fees. 

Registries where $openOptInDisabled$ is $true$ are invite only and require a signature for an EOA with the $SIGNER\_ROLE$ in order to join 
a data pool. There are two available methods for invitation-only user opt-in: $restrictedOptIn$ and $anonymousRestrictedOptIn$. The former method
requires that the recipient EOA be known in advance by the $SIGNER\_ROLE$ in order to construct the appropriate signature. The receiving EOA then 
becomes the only account that can call $restrictedOptIn$ with that signature. If the recipient EOA address is not known ahead of time, the $SIGNER\_ROLE$
can construct a signature for use with the $anonymousRestrictedOptIn$ method in which any EOA can submit the signature to that method (a single time) 
in order to opt-in. 

Support for both open and invitation-only opt-in flows makes the Protocol flexible to a variety of use-cases. However, end users are often hesitant to 
spent their own cryptocurrency in order to pay for transaction fees associated with a decentralized application or simply do not have the tokens 
necessary to do so. Additionally, requiring the user to spend their own assets to participate in the Protocol intoduces significant user friction and
adoption hurdles. Consent registries implement EIP-2772 compatible metatransaction capabilities to circumvent this issue. 

Meta transactions allow someone else to pay for a user's gas fees. All opt-in methods will implement support for metatransactions. This will offer the 
flexibility to have users pay for their own gas or have the businesses pay.  

\subsubsection{Identity Crumbs}
\label{section:Crumbs}

The Protocol introduces a special EIP-721 compatible registry, called the Crumbs Contract, to facilitate data wallet synchronization when an end user 
installs the client on a new device. When a user links a new EOA to their data wallet identity, the EOA encrypts their data wallet identity EOA (see section
\ref{section:DataWalletIdentity}) and stores the encrypted data in the token URI of an entry in the Crumbs contract. 

During a new client installation, the end user must simply link any EOA that has previously been linked to their data wallet identity in order for the data 
wallet to synchronize from their previously saved state that has been stored in the decentralized persistence layer of the data wallet network. The data wallet
checks if the account being linked owns a token in the Crumbs contract; if so, it reads the encrypted content of the token URI, decrypts the information, and loads 
the public-private key pair into memory. 

\subsubsection{EIP-20 Token}
\label{section:Token}
The Protocol includes a fungible utility token adhering to the EIP-20 standard. This token will be used for paying various fees required to leverage
the Protocol (like publishing a new consent registry) and will also be used for voting in the Protocol DAO. The associated voting mechanism that 
accompanies the possession of a token can be delegated to a different address without relinquishing ownership of the token

\subsubsection{Decentralized Autonomous Organization}
\label{section:ImplementationDAO}

The Protocol will include a decentralized autonomous organization (DAO) implementation. The DAO will be responsible for proposing and executing upgrades to 
the Protocol. The particular pattern used by the Protocol DAO is based on Curve Finance's DAO and will be implemented with OpenZeppelin libraries. 

Token holders (see section \ref{section:Token}), are responsible for the creation and execution of DAO proposals. At mainnet launch it is anticipated that one 
token will render one vote, though this too could be modified via a DAO proposal. Token holders will have to reach a pre-specified quorum of voting power in 
order to successfully create a proposal in the DAO task queue. Successful proposals will be subject to a delay of at least one block before they can be executed 
in order to prevent flash loan attacks. 
%----------------------------------------------------------------------------------------------------------------------------------------
\subsection{Off-Chain Components}
\label{section:OffChain}

\subsubsection{Data Wallet}
\label{section:DataWallet}

\input{DataWalletStructureTikz}

The data wallet is the primary client interface for end users to interact with the Protocol. It enables data ownership by facilitating user control 
and consent to the collection, storage, and usage of their data. The data wallet should provide the following functionality (as indicated 
by figure \ref{fig:DataWalletStructure}):

\begin{itemize}
  \item Ingestion and indexing of user data from the data wallet client deployment environment
  \item Identity and verifiable credential generation/management
  \item Query/Reward discovery and management
  \item A query engine for individualized data mining, insight processing and delivery
  \item A consent management interface and granular access control
  \item Secure storage of the data
\end{itemize}

To the user, the data wallet operates in a conceptually similar way to a conventional cryptocurrency wallet, but with a wider scope. 
Instead of key and account management of a blockchain account, a data wallet manages the storage, collection, and sharing of insights derived 
from user data. Data wallet functionality is form-factor agnostic (see section \ref{section:FormFactor}). However, browser extension and 
mobile applications are anticipated to be the primary channels for use.

\paragraph{User Data Aquisition Control Flow}
\label{section:AquisitionControlFlow}

\input{InsightControlFLowTikz}

The acquisition of insights from the data network begins at the consent contract factory (see section \ref{section:ConsentFactory}) where an organization 
publishes a consent registry. Data wallets belonging to end users, who have claimed a consent token via an invitation flow (section \ref{section:ConsentInvitations}),
detect $requestForData$ events from the associated consent registry. The event will reveal a CID which resolves to query definition file (section \ref{section:RequestForData}). 
The query execution layer of the data wallet will parse the query definition, construct the associated abstract syntax tree (AST), and apply the logic to the
data wallet persistence layer in a manner consistent with the conditions given by the user's on-chain permission settings (section \ref{section:UserPermissions}). This
control flow is outlined in figure \ref{fig:InsiteControlFlow}.

\paragraph{Storage}

Secure storage of data is crucial to allowing end users to own their data. The initial data wallet implementation produced by 
Snickerdoodle Labs exposes a modular storage interface capable of integrating with various storage provider technologies, 
such as Google Storage, Amazon S3, or decentralized options like the Ceramic Network.

It is also important to call out the wallet's storage of public-private key pairs. A data wallet should only store public keys 
and digital signatures associated with accounts linked to a user's data wallet, not private keys. 
Instead, we integrate with existing wallets (e.g., MetaMask) for key management.

\subsubsection{Collection}
The data wallet will also help users own their own data by allowing them to collect the data that they generate. This individualized data mining 
provides highly accurate data that the protocol allows users to easily monetize. The data the wallet collects has three important properties: 
explicit/implicit, first/third party, and authenticated/unauthenticated. %Not sure if there's a better term

% should be included but I'm not sure where or how
An important feature to highlight is that multiple addresses can be linked together in the same data wallet, including wallets for separate chains. 

% Not sure if this should be a paragraph or points
\paragraph{Explicit/Implicit}
The data wallet will either collect data through explicit or implicit means. Explicit data is data directly provided by the user, such as their name, 
age, or wallet address. In the initial version of the data wallet, the user will manually input this data; however, in the future, the data wallet 
could feature in-browser event capturing to passively collect this data (e.g., collecting information about what websites a user has visited).


Implicit data is data generated by user action. For example, using a user's wallet address to learn that they swapped specific tokens or have played a web3 game. 


\paragraph{First/Third Party}
Data collected can come from different sources. Specifically, first-party data comes directly from the user, and third-party data comes from 
someone other than the user. For example, if the user directly inputs their name, their name would be considered first-party data; if the 
user imports their name from the DMV, that would be third-party data.


\paragraph{Authenticated/Unauthenticated}
Data can be authenticated if its origin and validity can be verified through cryptographic means and unauthenticated if it can't. For example, 
a wallet address can be authenticated if we receive a signed message from that address. Third-party data can be authenticated if it has a known 
identity (e.g., the DMV's public key is known, and the data wallet receives data signed by that key). 

\subsubsection{Localized Processing} % listens to events and processes queries

The data wallet is a local application that stores the owner's data securely and processes computations locally. By collecting and 
securely storing user data locally, the data wallet guarantees data ownership to the user by never sharing it. Because computations 
are running locally, the owner ensures that only analysis they've given consent to can run on their data. Businesses also benefit 
from this model as they can leverage data-driven insights without worrying about the liability or infrastructure of storing raw, 
personal data. While the initial version of localized processing will be more limited, there is a myriad of ways we can modify this 
approach to add additional data safety and features (see section \ref{section:Future}). 

The wallet will learn what computations to run by listening to on-chain events. These events will be emitted on the Snickerdoodle Avalanche 
subnet and specify SDQL queries that include the computation instructions. If those data request events are from consent contracts that the 
user has opted into, the data wallet will process the data following the rules in the associated SDQL query. After the localized processing 
has been completed, the data wallet will send the insights to the specified endpoint, and the data owner will collect any rewards.


\subsubsection{Snickerdoodle Query Language} % maybe move to contracts 
\label{section:SDQL}

The Snickerdoodle Query Language will allow businesses to share their data requests with individual users in a transparent and auditable fashion. 
It will be structured as a simple JSON file containing information on the eligibility requirements, rewards, data to be collected, what processing 
to perform, and where to send processed data (see figure TODO). The allowed functions and syntax of the language will be enforced by the 
Snickerdoodle DAO and determined by token holders, which will allow for its continued development and the enforcement of user privacy. SDQL 
queries must be stored on a content-addressable distributed network, such as IPFS, to ensure tamper resistance. 

TODO FIGURE SHOWING QUERY + flesh out sections


\subsubsection{Rewards}

An essential part of the protocol is allowing individuals to get value from their rewards. The data wallet will facilitate this by allowing users 
to gain rewards for sharing their data. First, the data wallet helps people find rewards that are being offered. If a person goes to a website 
that's enabled the Snickerdoodle Protocol, they will see a pop-up and can connect their data wallet to check out possible rewards. Additionally, 
the data wallet will have a section showing available rewards.


Once the user has given consent to share data, the data wallet will also help the user manage their rewards. When they receive a query, they will 
be able to visualize the rewards offered in that query. The owner will also be able to set which addresses and chains they want to receive rewards. 
Additionally, the owner will be able to see the history of the rewards they've received and the data exchanged. 

\subsubsection{Authorization and Consent Management}

Enabling informed consent for data sharing is a core function of the Snickerdoodle Protocol. Consent is accomplished through the use of consent contracts 
(see section \ref{section:Contracts}). When a user wishes to consent to share data, they make a contract call and are issued a non-transferable consent 
token. To revoke consent, they can burn that token. The data wallet facilitates this by giving the owner's crypto wallet the appropriate transactions to 
sign. Once opted in, the data wallet will respond to all eligible queries from that pool unless more granular data permissions have been specified within 
the wallet. TODO granular permissions
%TODO granular permissions and opting out


For the initial version of the Snickerdoodle Protocol, we are not allowing users to choose what types of queries they can opt in and out of. A user giving 
consent to a company allows that company to run any query on their data. However, to maintain privacy, we limit queries' interactions with PII. If any 
queries touch-sensitive data, we will reduce the resolution of the insight, e.g., location data will only reveal the state or country, not the zip code. 
Additionally, part of the design of the data wallet is that it has the final say on what data is shared. Even if a user consents to a consent contract, a 
data wallet can theoretically choose not to respond, making it easy for future versions of the wallet to enable more granular privacy controls.

\subsubsection{Distributed Runtime Library}

The distributed runtime library is a core security feature of the data wallet. The wallet browser plugin will leverage iFrames to run plugins for data 
processing and wallet integrations. Using iFrames like this will isolate the data wallet's processing from the browser. To ensure the validity of these 
plugins, we will use the iFrame to enforce the legitimacy of the code by verifying certificates and checksums with the DAO allowlist. In this way, the 
user has a trustless way to verify their data is being processed and held safely.

\subsubsection{Queries}

Queries are responsible for specifying the who, what, and why of data processing. They allow data subscribers to define who they want data from, what type 
of computation to run, where to send the insight, and the reward for sending the insight. These queries must be content-addressed, so anyone can access the 
query and verify its validity. For the initial version of the Snickerdoodle Protocol, we will be publishing these queries on IPFS. In the future, the protocol 
will support queries hosted through other means.

TODO add info only allow queries if there are 30+ consent tokens given out

\subsubsection{Network \& Fees}

The Snickerdoodle Protocol will be deployed on the Snickerdoodle Network, which will be an EVM-compatible Avalanche Subnet. Our requirements for the 
network were we wanted a scalable network with low fees and EVM compatibility. Making an Avalanche Subnet fit these requirements (IDK more and IDK the how).

As mentioned in \ref{section:DoodleToken}, the native token of this subnet will be the Doodle and will be used for paying fees. Within the context 
of the protocol and managing data, gas fees will be paid for creating consent contracts, creating consent tokens, and emitting events (i.e., SDQL queries). 
Data subscribers will pay additional protocol fees to emit an SDQL query to access insights. This protocol fee will be proportional to the amount of data 
being accessed and can be calculated using the number of consent tokens tied to a specific contract.

\subsubsection{Rewards}

After sharing insights, the protocol will facilitate the transfer of rewards to the data owner. The owner can choose which wallet to send rewards to, not 
just the data wallet. In the initial version of the protocol, the insights/rewards transfer process will be trust-centric. When an owner responds to a 
query, they must trust that the business will send them the reward later. When subscribers send a reward, they need to trust that the user is sending 
over authentic insights. In future versions, we will enable atomic reward swaps. Atomic swaps allow insights to be shared at the same instance that a 
reward is received. We could easily add this functionality if the rewards are in DOODLE tokens, and zero-knowledge proofs could enable atomic swaps 
with different assets across chains (see \ref{section:AtomicSwaps}).

\subsubsection{Privacy}

A user receiving a consent token is how users signal their consent and a business emitting an event is how they signal they are interested in a particular 
insight. This transparency in knowledge does come with some privacy tradeoffs:
\begin{itemize}
  \item Everyone knows who is giving consent to a business
  \item Everyone knows what queries a business has run
\end{itemize}

These privacy trade-offs are acceptable for the initial version of the protocol. Our goals are to make sure users are able to give informed consent to the 
use of their data, data isn't revealed if this consent isn't given, insights rather than raw data are revealed if consent is given, and businesses will pay 
more to send queries to more people. The use of public consent tokens allows us to achieve these goals with an acceptable amount of privacy. It also gives 
the initial version of the protocol an interoperability bonus by allowing the consent tokens to integrate with existing web3 solutions. By making consent a 
public token that conforms to the ERC-721 standard, people can see what they've consented to through any application that shows owned NFTs. In a future 
version, we could create ways to hide this information. For example, an anonymized consent token could be minted while the data wallet listens and responds 
to events. %TODO maybe reference future


It's also worth pointing out that this decision doesn't reduce the privacy of individuals too much more than if we didn't use public consent tokens. Accepting 
rewards linked to a business reveals that consent has been given to that business (i.e., Alice having a Bob's Business NFT means everyone knows Alice consented 
to provide information to Bob's Business). Additionally, there will be a limited number of subscribers for the initial release, all of which will be web3 
companies. This limitation reduces the information revealed by owning a token. Holding a consent token for a particular business indicates interaction with 
the business and the willingness to adopt a web3 product for a reward. The former is already public knowledge by looking at the blockchain's history, and the 
latter is a common trait of web3 users.


We are also ok reducing businesses' privacy by making what queries are run public. We feel that more transparency in what information businesses are interested 
in gives people more insight into whether they want to share their data and reduces the societal risk of companies running complex insights that hurt the 
commonwealth. If we need to add more privacy to businesses, we could add it in future versions by creating private query cohorts. See section \ref{section:Future} 
for a discussion on future changes to the protocol that can increase privacy.

\subsection{Data Ingestion Provider} % name for role?

Data ingestion is the final stage of the insight aggregation process that allows subscribers to access the insights generated at the data wallet layer. 
This section will detail how the subscriber can see processed data from the user's data wallet. Any entity can assume the role of data ingestion service 
as long as they follow the protocol and are accepted by the DAO. Snickerdoodle Labs will offer the first data ingestion provider, which will be tied to 
a SaaS product. 

\subsubsection{Ingestion}
The final step of the protocol is for data wallets to send insights from their queries to an endpoint. This endpoint is the ingestion provider and is 
specified in the SDQL query. Ingestion is an integral part of the protocol as it allows businesses to see the insights they have paid for.

Once the ingestion provider receives the insight, it must store the insights events and manage access. They can also provide any additional services 
they want on top of this, such as providing a dashboard to view insights. There is no way to mathematically guarantee any ingestion provider is behaving 
honestly or doesn't have any bugs. The lack of a strong guarantee means that actors in the protocol have to put some trust in ingestion providers. The 
Snickerdoodle Protocol reduces this problem by only allowing ingestion services that the DAO has approved. Any provider has to be trustworthy enough for 
the DAO to vote them in; if a provider is revealed to be a bad or incompetent actor, they can be voted out. For a deeper discussion of data safety 
concerns with ingestion providers, see section \ref{section:IngestionDataSafety}.

\subsubsection{Data Safety}
\label{section:IngestionDataSafety}

The initial version of the protocol requires trust in the Insight Platform to maintain data safety. As discussed above, there are no guarantees that the 
ingestion provider acts honestly. The ingestion provider can see, delete, and sell any insights they receive. Additionally, because the initial version 
of the wallet only implements simple anonymization techniques, ingestion providers could identify who shared the insight and reconstruct the raw data. 


To reduce these privacy risks, the initial version of the protocol will only allow queries that don't reveal PII, and future versions will add anonymization 
techniques to insight computations (see section \ref{section:Future}). To reduce the risk of malicious ingestion services, the Snickerdoodle DAO will maintain 
an allowlist of trusted actors. Additionally, Snickerdoodle Labs will be providing an ingestion service. The Snickerdoodle Ingisht Service is a SaaS product 
that will enforce data safety and has a massive financial and legal incentive to behave honestly.

\subsubsection{SDL Insight Platform}
\label{section:InsightService}

The Snickerdoodle Insight Platform will be our SaaS product offering for interaction with the Snickerdoodle Protocol. The Insight Platform will provide an 
ingestion service and help manage interaction with the rest of the protocol. This includes support for creating and managing queries, consent contracts, 
rewards, and website integrations. The Insight Platform will also provide an analytics dashboard to help businesses see their insights.