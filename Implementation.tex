\section{Implementation}
\label{section:Implementation}
%------------------------------------------------------------------------------------------
\subsection{Architecture}
\label{section:Architecture}

This section will discuss the implementation and design decisions of the Protocol. The Protocol has three primary 
components: a data wallet implementation, an on-chain data control plane, and aggregation service providers.

The data wallet is a software client that implements functionality which enables end-users to collect, index, and store their data as well as participate 
in the decentralized data network, see section \ref{section:DataWallet}. Organizations (consumers of data insights) will be able 
to query populations of data wallets through an on-chain control plane that adheres to a publish-subscribe (pub-sub) pattern, see section \ref{section:OnChain}. 

At the core of the control plane is an upgradable contract factory which produces independent instances of an EIP-721 compatible consent registry. 
Consent tokens claimed from these consent contracts are non-transferable but can be burned by the recipient. Claiming a consent token denotes a data 
wallet user's consent to participate in network queries in return for rewards (which may or may not be web3 digital assets). 

Data wallets receive queries via EVM events emitted from consent registry contracts they have claimed tokens in. These events contain metadata encoding
instructions (written in SDQL) to run computations on the data stored in the data wallet to produce insights. Once a data wallet has produced
the requested insight, it performs a digital "handshake" with the aggregation service provider specified in the query metadata such that the 
data wallet owner receives a reward while the requesting organization receives the insight, see figure \ref{fig:OnChainOffChain}. 

User consent and data flow is thus orchestrated in a distributed manner by the Protocol. It is also worth highlighting that while 
Snickerdoodle Labs will develop service infrastructure for the protocol (such as producing a data wallet and a SaaS product offering 
for enterprise participation in the protocol), the protocol itself is, however, permissionless and open so that anyone could implement a 
data wallet client or act as an aggregation service provider if the specifications of the protocol is adhered to. 
%----------------------------------------------------------------------------------------------------------------------------------------
\subsection{On-Chain Components}
\label{section:OnChain}

\subsubsection{Consent Contract Factory}
\label{section:ConsentFactory}

\input{ConsentContractPubSubTikz}
The on-chain components of the Protocol functions as a decentralized, permissionless data control plane. It specifically implements a publish-subscribe pattern
in which organizations publish new instances of an EIP-721 compatible consent registry (see \ref{section:ConsentContract}), and end-users 
subscribe to the registries by claiming a non-transferrable consent token. The publishing action is performed via the Protocol's upgradable 
consent contract factory, see figure \ref{fig:ConsentFactory}. The factory contract will be the entrypoint to the Protocol for new insight 
consumers, since a consent registry is required to communicate with the network of data wallets. 

\input{ContractUpgradePatternTikz}
The consent contract factory exists as a utility for insight consumers to create new consent registries. The factory is implemented with an 
upgradable beacon pattern, see figure \ref{fig:UpgradePattern} to enable gas-efficient deployments and to allow for seamless extensions of functionality
via DAO proposals. 

\paragraph{Factory Pattern}
The factory pattern defines a smart contract which is responsible for creating other contracts. The Protocol uses the factory 
pattern to simplify the deployment of new consent registries and to give new insight consumers a single point of entry into the data network. 

\paragraph{Upgradable Beacon Pattern}
\label{section:BeaconPattern}

Consent registries are deployed as proxy contract instances that reference an upgradable beacon contract to obtain the correct address to delegate
function calls, see \ref{fig:UpgradePattern}. This upgrade pattern compliments the factory pattern by allowing for very gas-efficient deployments 
of new proxy instances. Proxy contracts only store storage variables and a pointer to their designated upgradable beacon contract. The upgradable 
beacon contract points to an implementation contract. The implementation contract contains all functions implementations as well as the storage
variable declarations that proxy contracts copy. 

A Protocol upgrade to the consent registry functionality requires that a new implementation contract first be deployed to the blockchain. Then a 
DAO proposal must be initiated to point the upgradeable beacon to the new implementation address. All previously deployed and newly created proxy 
consent contracts inherit the functionality (and any new storage variables) defined in the new contract.

The upgradeability pattern is based on EIP-1967 (CITE) and is implemented through OpenZeppelin libraries (CITE).

\subsubsection{Consent Registries}
\label{section:ConsentContract}

\input{OnChainOffChainInteractionTikz}
Consent registry contracts are the primary on-chain mechanism by which insight consumers interact with with data wallet end-users. Consent 
registries allow organizations to create data pools by serving as an on-chain data structure that holds metadata regarding the conditions 
under which data is to be collected and used as well as a cryptographically verifiable list of externally owned accounts (EOAs) that have
have given consent to participate in the data pool. 

Consent registries expose an EIP-721 compatible interface. This is for developer integration convenience as it allows consent registries to 
be readable by most existing NFT indexing services (such as Snowtrace). Consent is denoted by ownership of a non-transferrable consent NFT 
(non-fungible token) which can be burned by the owner at any time. 

\paragraph{Request for Data Events}

After an organization has published a consent registry via the Consent Contract Factory (see \ref{section:ConsentFactory}), the organization can emit
EVM events by calling a special function, $requestForData$, which takes a content identifier (CID) as its only input. This CID is used to 
retrieve the request specifications from a suitable content addressable network (like IPFS) that the data wallets will process. Data wallets can detect 
past $requestForData$ events by constructing EVM query filters and requesting all EVM logs that match those filters. Thus consent registries offer a 
tamper-resistant communication layer between organizations and participants in their data pools. See figure \ref{fig:OnChainOffChain}

Consent registries also specify a $queryHorizon$ which inform data wallets of the oldest block number to search for these events. This variable is 
first initialized to the block number that the proxy contract was deployed from the contract factory and can be updated by an EOA with the $DEFAULT\_ADMIN\ROLE$
to a later blocknumber (but cannot be changed to an earlier block number). Setting a reasonable value for $queryHorizon$ is important since many RPC providers
only allow for query filters to search a limited history of the chain.

\paragraph{User Data Permissioning}

A $requestForData$ event indicate that it requires access to multiple attributes indexed by the user's data wallet client, such as country of origin, age,
on-chain contracts they have interacted with using their linked EOA asset account. However, users can set granular permissions regarding their indexed data 
attributes. 

Every consent token issued from a consent registry has an associated set of binary $aggreementFlags$. There are 256 flags in total (the size of an EVM word) 
though not all flags will be assigned to specific attributes at Protocol launch, leaving room for customization. Only the owner of a consent token can 
update the granular permissions denoted by the token's $agreementFlag$s. The availability of granular consent data on-chain allows organizations to better 
understand what kinds of insights they will be able to obtain from their data pool before calling $requestForData$. 

\paragraph{Consent Invitations}
\label{section:ConsentInvitations}

\input{Web3PopupTikz}
The Protocol allows for the decentralized, trustless, and tamper-resistant detection of events that should be presented to the data wallet 
end user in a format appropriate for the environment. In a web browser setting, the data wallet detects the current active URL, and via DNS 
over HTTPS (DoS) queries the TXT records associated with the apex domain. If the TXT record contains a reference to a Protocol consent 
contract address, the data wallet then fetches the URLs registered in the contract metadata on the blockchain and cross-references the domains 
listed from the contract to the current URL. If the data wallet detects that the current URL is included in the domains listed in the 
consent contract, the data wallet will inject a popup into the browser DOM. The content of the popup is fetched from an appropriate content 
addressable network (such as IPFS) and the content address is given by the consent contract metadata. The Web3 popup protocol can be 
extended to other environments including mobile browsing environments, VR experiences, gaming consoles, etc as indicated by figure \ref{fig:PopupProtocol}. 

\paragraph{Meta Transactions}

Meta transactions allow someone else to pay for a user's gas fees. Every state-changing transaction (i.e., any transaction with a gas fee) will 
have meta transactions enabled. We did this to give us the flexibility to have users pay for their own gas or have Snickerdoodle Labs or another 
business pay. In the initial version, we don't want users to pay for their gas costs as it creates a bad user experience. Users will need to add 
the native gas token to their Snickerdoodle Wallets, which often involves buying a new token, bridging that token, and paying gas along the way. 
Also, people don't like spending money on gas costs (do I need to cite?). 

Meta transactions follow the ERC2771 standard and are implemented using OpenZepplin's libraries (CITE).


\subsubsection{Doodle Token}
\label{section:DoodleToken}
% general description
% ERC-20 
% why 
%   token for governance and fees
% how 
%      some kind of wrapping for subnet
%       3 implementations bc we plan on subnet (2 on subnet)
%           native DOODLE (not ERC-20) -- wrap with DAO contract
%           WDOODLE (ERC-20) so you can vote in DAO 
%           ----- not subnet below -----
%           vanilla DOODLE (ERC-20) linked on C-Chain+others for other bridging ERC-20 infrastructure 
%       If on a different EVM network:
%              Just DOODLE (ERC-20)
%           vanilla DOODLE (ERC-20) linked for bridging ERC-20 infrastructure 
%   ERC-20 + openzepplin
The token that powers the protocol will be the Doodle Token. This token will be used for paying Snickerdoodle Subnet gas fees, paying Snickerdoodle Protocol 
fees, and voting in the Snickerdoodle DAO. 

The Doodle Token will come in several forms to support these use cases. First, it is a native non-ERC-20 token of the Snickerdoodle subnet. This native 
token will be used to pay both gas and protocol fees. The second form will be an ERC-20 token, Wrapped Doodle (WDOODLE), representing votes in the DAO. 
The DAO itself will have a contract that can convert DOODLE and WDOODLE back and forth. Lastly, there will be vanilla ERC-20 implementations of the DOODLE 
on other EVM chains to enable bridging so the DOODLE can be used with other standard ERC-20 infrastructure. We will use openZepplin Libraries to implement 
these contracts. 


\subsubsection{DAO}
\label{section:ImplementationDAO}
% general description
% why
%   Need decentralized way to control allowed queries, certificate authorities, allow/ban lists for known good/bad actors
% how
%   openzeplin curve finance
The DAO is the decentralized governance mechanism of the Snickerdoodle Protocol. The DAO creates a decentralized way to manage allowed queries, acts as a 
certificate authority for the code distribution, and controls an allow/ban list of good/bad actors. The DAO will be modeled on Curve Finance's DAO (CITE) 
and will be implemented with OpenZepplin libraries (CITE). 

\subsubsection{Queries}
% SDQL queries (add visuals)
% need to be content-addressed (IPFS, Arweave, etc)
% will be published with the request for verifiability
Queries are responsible for specifying the who, what, and why of data processing. They allow data subscribers to define who they want data from, what type 
of computation to run, where to send the insight, and the reward for sending the insight. These queries must be content-addressed, so anyone can access the 
query and verify its validity. For the initial version of the Snickerdoodle Protocol, we will be publishing these queries on IPFS. In the future, the protocol 
will support queries hosted through other means.

TODO add info only allow queries if there are 30+ consent tokens given out

\subsubsection{Network \& Fees}
% general description
%   One an avax subnet
%   ARE WE SUBNET OR ON ANOTHER NETWORK
% why
%   Want scalability and lower fees
%   EVM
% how
%   Still 

% fees
%   fee in native currency for any statechanging action we take (gas)
%       (creating contract, consent tokens, emitting events)
%   extra fee in DOODLEs for accessing insights
%   rewards
The Snickerdoodle Protocol will be deployed on the Snickerdoodle Network, which will be an EVM-compatible Avalanche Subnet. Our requirements for the 
network were we wanted a scalable network with low fees and EVM compatibility. Making an Avalanche Subnet fit these requirements (IDK more and IDK the how).

As mentioned in \ref{section:DoodleToken}, the native token of this subnet will be the Doodle and will be used for paying fees. Within the context 
of the protocol and managing data, gas fees will be paid for creating consent contracts, creating consent tokens, and emitting events (i.e., SDQL queries). 
Data subscribers will pay additional protocol fees to emit an SDQL query to access insights. This protocol fee will be proportional to the amount of data 
being accessed and can be calculated using the number of consent tokens tied to a specific contract.

\subsubsection{Rewards}
% Initial version will be trust centric
%   Get query give data, trust get reward, trust get valid data
% Atomic swaps / escrow something to look at
%   Simple way would be DOODLEs as only reward that this works with
%   Future look into ZK transfers
After sharing insights, the protocol will facilitate the transfer of rewards to the data owner. The owner can choose which wallet to send rewards to, not 
just the data wallet. In the initial version of the protocol, the insights/rewards transfer process will be trust-centric. When an owner responds to a 
query, they must trust that the business will send them the reward later. When subscribers send a reward, they need to trust that the user is sending 
over authentic insights. In future versions, we will enable atomic reward swaps. Atomic swaps allow insights to be shared at the same instance that a 
reward is received. We could easily add this functionality if the rewards are in DOODLE tokens, and zero-knowledge proofs could enable atomic swaps 
with different assets across chains (see \ref{section:AtomicSwaps}).

\subsubsection{Privacy}
% This is how users signal consent
% Trade-off of public knowledge with utility
%   Privacy of everyone knowing who's giving consent to Shrapnel
%   Everyone knows what queries Shrapnel is interested in
%       Potential loss of competitive edge + buisisness privacy
%           - Reason is Philosophical argument (transparency, control, risk reduction)
%           - Need future section about creating private query cohorts
% Reasons why we are ok with tradeoffs made for now
A user receiving a consent token is how users signal their consent and a business emitting an event is how they signal they are interested in a particular 
insight. This transparency in knowledge does come with some privacy tradeoffs:
\begin{itemize}
  \item Everyone knows who is giving consent to a business
  \item Everyone knows what queries a business has run
\end{itemize}

These privacy trade-offs are acceptable for the initial version of the protocol. Our goals are to make sure users are able to give informed consent to the 
use of their data, data isn't revealed if this consent isn't given, insights rather than raw data are revealed if consent is given, and businesses will pay 
more to send queries to more people. The use of public consent tokens allows us to achieve these goals with an acceptable amount of privacy. It also gives 
the initial version of the protocol an interoperability bonus by allowing the consent tokens to integrate with existing web3 solutions. By making consent a 
public token that conforms to the ERC-721 standard, people can see what they've consented to through any application that shows owned NFTs. In a future 
version, we could create ways to hide this information. For example, an anonymized consent token could be minted while the data wallet listens and responds 
to events. %TODO maybe reference future


It's also worth pointing out that this decision doesn't reduce the privacy of individuals too much more than if we didn't use public consent tokens. Accepting 
rewards linked to a business reveals that consent has been given to that business (i.e., Alice having a Bob's Business NFT means everyone knows Alice consented 
to provide information to Bob's Business). Additionally, there will be a limited number of subscribers for the initial release, all of which will be web3 
companies. This limitation reduces the information revealed by owning a token. Holding a consent token for a particular business indicates interaction with 
the business and the willingness to adopt a web3 product for a reward. The former is already public knowledge by looking at the blockchain's history, and the 
latter is a common trait of web3 users.


We are also ok reducing businesses' privacy by making what queries are run public. We feel that more transparency in what information businesses are interested 
in gives people more insight into whether they want to share their data and reduces the societal risk of companies running complex insights that hurt the 
commonwealth. If we need to add more privacy to businesses, we could add it in future versions by creating private query cohorts. See section \ref{section:Future} 
for a discussion on future changes to the protocol that can increase privacy.
%----------------------------------------------------------------------------------------------------------------------------------------
\subsection{Data wallet}
\label{section:DataWallet}

\input{DataWalletStructureTikz}

\input{InsightControlFLowTikz}

The data wallet is the primary end-user client interface within the Protocol. It enables data ownership by facilitating user control 
and consent to the collection, storage, and usage of their data. The data wallet should provide the following functionality:
\begin{itemize}
  \item Secure storage of the data
  \item A query engine for individualized data mining
  \item Aggregation and indexing of user data. This should include on-chain data, data from traditional third-party sources, or digital identities.
  \item A consent management interface for granular access control
  \item Localized insight processing and submission
  \item Reward discovery and management
  \item Identity and verifiable credential management
\end{itemize}

% Overview
%   Control over-collection and storage -- ownership
%                           Mention idea of automated data fiduciary
%   Conceptually what is a data wallet?   
%       - Custodial entity for user data
%       - Collection engine for individualized user data mining
%       - Aggregation of implicit and external data (ID card metaphor) -- adding example
%           - DID
%       - Consent manager
%   Summary of exactly what the wallet does6

To the user, the data wallet operates in a conceptually similar way to a conventional cryptocurrency wallet, but with a wider scope. 
Instead of key and account management of a blockchain account, a data wallet manages the storage, collection, and sharing of insights derived 
from user data. The initial form factor will be a stand-alone browser extension with plans to expand the form factor in future versions (see 
section \ref{section:FormFactor}). By providing this localized control interface, the data wallet will give individuals greater control 
of their own data and allow for participation in the Protocol.

% rest is how wallet does it 
%   - traditional crypto wallet form factor -- picture if we have design
%       - chrome extension (planned mobile app / headless)
%   - Holds consent tokens, holds explicit data, collects implicit data
%       - listens for queries and performs localized data operations

\subsubsection{Storage}
%   LINK MULTIPLE USER ACCOUNTS TO SINGLE AGENT. CAN GIVE INFO ABOUT CROSS CHAIN ACTIVITY WITHOUT REVEALING LINKAGE
% data wallet holds data -> how do we collect and store it?
% Browser local storage
%   Key value pairs
%   Simple in-memory database that by default is limited ~5MB
%           modern browsers allow use to ask for more than just that
%   Future: have more sophisticated options for in browser data base (e.g., indexDB)
% Collect
%   Manually imputed data (explicit)
%   Implicit (eg on-chain)
%       -Need user to sign their wallet address
%           LINK MULTIPLE WALLETS IN SAME PLACE
%       -Use indexer to find queries
%       -(future) in-browser event capturing
%   Verified / third party data (future)
Secure storage of data is crucial to allowing people to own their data. The initial data wallet implementation produced by 
Snickerdoodle Labs will expose a modular storage interface capable of integrating with various storage provider technologies, 
such as Google Storage, Amazon S3, or decentralized options like the Ceramic Network.

It is also important to call out the wallet's storage of public-private key pairs. A data wallet should only store public keys 
and digital signatures associated with accounts linked to a user's data wallet, not private keys. 
Instead, we integrate with existing wallets (e.g., MetaMask) for key management.


\subsubsection{Collection}
The data wallet will also help users own their own data by allowing them to collect the data that they generate. This individualized data mining 
provides highly accurate data that the protocol allows users to easily monetize. The data the wallet collects has three important properties: 
explicit/implicit, first/third party, and authenticated/unauthenticated. %Not sure if there's a better term

% should be included but I'm not sure where or how
An important feature to highlight is that multiple addresses can be linked together in the same data wallet, including wallets for separate chains. 

% Not sure if this should be a paragraph or points
\paragraph{Explicit/Implicit}
The data wallet will either collect data through explicit or implicit means. Explicit data is data directly provided by the user, such as their name, 
age, or wallet address. In the initial version of the data wallet, the user will manually input this data; however, in the future, the data wallet 
could feature in-browser event capturing to passively collect this data (e.g., collecting information about what websites a user has visited).


Implicit data is data generated by user action. For example, using a user's wallet address to learn that they swapped specific tokens or have played a web3 game. 


\paragraph{First/Third Party}
Data collected can come from different sources. Specifically, first-party data comes directly from the user, and third-party data comes from 
someone other than the user. For example, if the user directly inputs their name, their name would be considered first-party data; if the 
user imports their name from the DMV, that would be third-party data.


\paragraph{Authenticated/Unauthenticated}
Data can be authenticated if its origin and validity can be verified through cryptographic means and unauthenticated if it can't. For example, 
a wallet address can be authenticated if we receive a signed message from that address. Third-party data can be authenticated if it has a known 
identity (e.g., the DMV's public key is known, and the data wallet receives data signed by that key). 

\subsubsection{Localized Processing} % listens to events and processes queries
% What are advantages of this model? 
     % for individuals
     % for businesses
     % future benefits of localized processing in another section% 
% How is this done? 
%   - SDQL querying
%   - Listens for events on chain
%   - Using existing wallets (e.g., metamask) to manage keys. NOT STORING PRIVATE KEYS RN
%   - reference future improvements
% Increased Data Safety
The data wallet is a local application that stores the owner's data securely and processes computations locally. By collecting and 
securely storing user data locally, the data wallet guarantees data ownership to the user by never sharing it. Because computations 
are running locally, the owner ensures that only analysis they've given consent to can run on their data. Businesses also benefit 
from this model as they can leverage data-driven insights without worrying about the liability or infrastructure of storing raw, 
personal data. While the initial version of localized processing will be more limited, there is a myriad of ways we can modify this 
approach to add additional data safety and features (see section \ref{section:Future}). 

The wallet will learn what computations to run by listening to on-chain events. These events will be emitted on the Snickerdoodle Avalanche 
subnet and specify SDQL queries that include the computation instructions. If those data request events are from consent contracts that the 
user has opted into, the data wallet will process the data following the rules in the associated SDQL query. After the localized processing 
has been completed, the data wallet will send the insights to the specified endpoint, and the data owner will collect any rewards.


\subsubsection{Snickerdoodle Query Language} % maybe move to contracts 
\label{section:SDQL}
% data app
% SDQL exists to run computations on local data
% Structure
%   - json file
%       - Qualifying
%           mention privacy issue here that's in future -- qualify + reward can it reveal
%       - Rewards
%       - ? who indicated ?
%       - What function to run
%   - maybe discuss CID
%   - example
% why
%   - Decentralized  DAO control of quries
%   - Tamper Resistence
%       - guarentee what we are asking is what is being delivered
%   - CONTENT ADDRESSING
%   - json simplicity
The Snickerdoodle Query Language will allow businesses to share their data requests with individual users in a transparent and auditable fashion. 
It will be structured as a simple JSON file containing information on the eligibility requirements, rewards, data to be collected, what processing 
to perform, and where to send processed data (see figure TODO). The allowed functions and syntax of the language will be enforced by the 
Snickerdoodle DAO and determined by token holders, which will allow for its continued development and the enforcement of user privacy. SDQL 
queries must be stored on a content-addressable distributed network, such as IPFS, to ensure tamper resistance. 

TODO FIGURE SHOWING QUERY + flesh out sections


\subsubsection{Rewards}
% Rewards
% - See / find rewards being offered 
%       - Websites integrate SDL
%       - Market place of available rewards
% - Manage their rewards
%       - See rewards offered by query
%       - History of accepted rewards
%       - Which wallets rewards should be sent to 

An essential part of the protocol is allowing individuals to get value from their rewards. The data wallet will facilitate this by allowing users 
to gain rewards for sharing their data. First, the data wallet helps people find rewards that are being offered. If a person goes to a website 
that's enabled the Snickerdoodle Protocol, they will see a pop-up and can connect their data wallet to check out possible rewards. Additionally, 
the data wallet will have a section showing available rewards.


Once the user has given consent to share data, the data wallet will also help the user manage their rewards. When they receive a query, they will 
be able to visualize the rewards offered in that query. The owner will also be able to set which addresses and chains they want to receive rewards. 
Additionally, the owner will be able to see the history of the rewards they've received and the data exchanged. 

\subsubsection{Authorization and Consent Management}
% How will the wallet know what data to share?
%   - Consent tokens / opt-ins
%   - granular permissions?
%   - opt-outs
% if possible add pictures
Enabling informed consent for data sharing is a core function of the Snickerdoodle Protocol. Consent is accomplished through the use of consent contracts 
(see section \ref{section:Contracts}). When a user wishes to consent to share data, they make a contract call and are issued a non-transferable consent 
token. To revoke consent, they can burn that token. The data wallet facilitates this by giving the owner's crypto wallet the appropriate transactions to 
sign. Once opted in, the data wallet will respond to all eligible queries from that pool unless more granular data permissions have been specified within 
the wallet. TODO granular permissions
%TODO granular permissions and opting out


For the initial version of the Snickerdoodle Protocol, we are not allowing users to choose what types of queries they can opt in and out of. A user giving 
consent to a company allows that company to run any query on their data. However, to maintain privacy, we limit queries' interactions with PII. If any 
queries touch-sensitive data, we will reduce the resolution of the insight, e.g., location data will only reveal the state or country, not the zip code. 
Additionally, part of the design of the data wallet is that it has the final say on what data is shared. Even if a user consents to a consent contract, a 
data wallet can theoretically choose not to respond, making it easy for future versions of the wallet to enable more granular privacy controls.

\subsubsection{Distributed Runtime Library}
% Distributed runtime library
%   - Wallet is browser plugin with iFrame
%   - security benefits
%   - upgradability
%       - want to change SDQL push code into iFrame
%       - DAO controls upgrade process (see if patent before publishing paper)

% Wallet is going to be standard browser plugin with iFrame
%.   Describe why
%.   Describe benefits 
% Describe security 
% Mention future section 
The distributed runtime library is a core security feature of the data wallet. The wallet browser plugin will leverage iFrames to run plugins for data 
processing and wallet integrations. Using iFrames like this will isolate the data wallet's processing from the browser. To ensure the validity of these 
plugins, we will use the iFrame to enforce the legitimacy of the code by verifying certificates and checksums with the DAO allowlist. In this way, the 
user has a trustless way to verify their data is being processed and held safely.

\subsubsection{Decentralized certificate authority}
\label{section:DAO}
% checksums and certs enforced by the DAO
%   - accepted logic determined by governance
%   - integrity of the plugins determined by DAO registry
% How are we building this?
%   - DAO is based on compound finance + solidity openzeplin
%   - ERC-721 registry / certificate
%       - Token URI holds the checksum + valid domain name
%   - Registry itself has access control so DAO is only one who can add and remove 
The DAO will maintain an allowlist of authorized plugins determined by governance. This allowlist will ensure that accepted logic is determined by the token 
holders and enforce the integrity of those plugins at the execution level. The DAO is based on Compound Finance's DAO and built using OpenZeppelin libraries. 
The certificate registry will be implemented as an ERC-721 contract where the token URI holds the checksum and domain name. The DAO will be the only party 
authorized to modify this registry. See section \ref{section:TokenDAO} for more details on the DAO.
%reference DAO

\subsection{Data Ingestion Provider} % name for role?
% How we allow data subscribing and apps
% execution role for data applications (SDQL queries)
% can be run by any party supporting protocol interface
%   - SDL will offer SaaS solution
% will be the endpoint of the SDQL query for data aggregation
% highlight that anyone can make their own
Data ingestion is the final stage of the insight aggregation process that allows subscribers to access the insights generated at the data wallet layer. 
This section will detail how the subscriber can see processed data from the user's data wallet. Any entity can assume the role of data ingestion service 
as long as they follow the protocol and are accepted by the DAO. Snickerdoodle Labs will offer the first data ingestion provider, which will be tied to 
a SaaS product. 

\subsubsection{Ingestion}
The final step of the protocol is for data wallets to send insights from their queries to an endpoint. This endpoint is the ingestion provider and is 
specified in the SDQL query. Ingestion is an integral part of the protocol as it allows businesses to see the insights they have paid for.



Once the ingestion provider receives the insight, it must store the insights events and manage access. They can also provide any additional services 
they want on top of this, such as providing a dashboard to view insights. There is no way to mathematically guarantee any ingestion provider is behaving 
honestly or doesn't have any bugs. The lack of a strong guarantee means that actors in the protocol have to put some trust in ingestion providers. The 
Snickerdoodle Protocol reduces this problem by only allowing ingestion services that the DAO has approved. Any provider has to be trustworthy enough for 
the DAO to vote them in; if a provider is revealed to be a bad or incompetent actor, they can be voted out. For a deeper discussion of data safety 
concerns with ingestion providers, see section \ref{section:IngestionDataSafety}.

\subsubsection{Data Safety}
\label{section:IngestionDataSafety}
% Lots of trust into aggreator
% lack of anonymization
% data security
% leak risk
% legal compliance
The initial version of the protocol requires trust in the Insight Platform to maintain data safety. As discussed above, there are no guarantees that the 
ingestion provider acts honestly. The ingestion provider can see, delete, and sell any insights they receive. Additionally, because the initial version 
of the wallet only implements simple anonymization techniques, ingestion providers could identify who shared the insight and reconstruct the raw data. 


To reduce these privacy risks, the initial version of the protocol will only allow queries that don't reveal PII, and future versions will add anonymization 
techniques to insight computations (see section \ref{section:Future}). To reduce the risk of malicious ingestion services, the Snickerdoodle DAO will maintain 
an allowlist of trusted actors. Additionally, Snickerdoodle Labs will be providing an ingestion service. The Snickerdoodle Ingisht Service is a SaaS product 
that will enforce data safety and has a massive financial and legal incentive to behave honestly.

\subsubsection{SDL Insight Platform}
\label{section:InsightService}
% in-depth on SDL SaaS product
%   - data aggregation
%   - managed querying
%   - pool management
%   - pinning service / content addressing
% Dashboard / analytics
%   Maybe plugin dashboards
The Snickerdoodle Insight Platform will be our SaaS product offering for interaction with the Snickerdoodle Protocol. The Insight Platform will provide an 
ingestion service and help manage interaction with the rest of the protocol. This includes support for creating and managing queries, consent contracts, 
rewards, and website integrations. The Insight Platform will also provide an analytics dashboard to help businesses see their insights.