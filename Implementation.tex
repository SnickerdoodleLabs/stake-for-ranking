\section{Implementation}
\label{section:Implementation}
TODO FIGURE SHOWING OVERALL FLOW
        may want to have additional images of wallet

% High-level overview

% diagram of protocol flow
%   - how does data move?
%   Query publishing -> processing -> endpoint called -> data ingestion

This section will discuss the initial implementation and design decisions of the Snickerdoodle protocol. The protocol has three primary components: data wallet, consent contracts, and data ingestion providers.

At a high level, the data wallet will allow people to collect and store their data and participate in the Snickerdoodle Network. Businesses will be able to query this data by creating consent contracts and submitting SDQL queries. Data wallets will receive these queries and follow their instructions to run computations on their data to create insights. Once a data wallet receives a reward from the business, it will send that insight to a data ingestion service for the business to see. TODO add diagram

This flow will be orchestrated in a distributed manner by the Snickerdoodle Protocol. Consent contracts will manage user consent and data requests. These contracts are created by businesses and facilitate consent by issuing non-transferable consent tokens. The protocol is governed and managed by the Snickerdoodle DAO. 

It is also worth highlighting that while Snickerdoodle Labs will be providing service infrastructure for the protocol (such as making a data wallet and a SaaS product offering for enterprise participation in the protocol), the protocol itself is permissionless and anyone could act as a data ingestion provider if the protocol is followed properly. Data ingestion is simply another role for any actor within the system to assume.

\subsection{Data wallet}
\label{section:DataWallet}
% This is how the individual interacts within data economy
%      Method to collect, consent, store, share, manage
%.     in a safe fashion
%

\input{DataWalletStructureTex}

The data wallet is the primary end-user client interface within the Snickerdoodle protocol. It enables data ownership by helping the user control and consent to the collection, storage, and usage of their data. The data wallet will facilitate this by providing the following functionality:
\begin{itemize}
  \item Secure storage of the data
  \item A collection engine for individualized data mining
  \item Aggregation of implicit and external data. This aggregation could include the aggregation of on-chain data, data from third-party sources, or digital identities.
  \item A consent management interface
  \item Localized data processing and submission
  \item Reward discovery and management
  \item Identity Management
\end{itemize}

\input{InsightControlFLowTikz}

% Overview
%   Control over-collection and storage -- ownership
%                           Mention idea of automated data fiduciary
%   Conceptually what is a data wallet?   
%       - Custodial entity for user data
%       - Collection engine for individualized user data mining
%       - Aggregation of implicit and external data (ID card metaphor) -- adding example
%           - DID
%       - Consent manager
%   Summary of exactly what the wallet does



To the user, the data wallet operates in a conceptually similar way to a conventional cryptocurrency wallet, but with a wider scope. Instead of key and account data management, a data wallet manages the storage, collection, and sharing of all user data. The initial form factor will be a browser extension with plans to expand the form factor in future versions (see section \ref{section:FormFactor}). By providing this localized control interface, the data wallet will give individuals greater control of their own data and allow for participation in the Snickerdoodle protocol.

(TODO add a figure here illustrating form factor)

% rest is how wallet does it 
%   - traditional crypto wallet form factor -- picture if we have design
%       - chrome extension (planned mobile app / headless)
%   - Holds consent tokens, holds explicit data, collects implicit data
%       - listens for queries and performs localized data operations

\subsubsection{Storage}
%   LINK MULTIPLE USER ACCOUNTS TO SINGLE AGENT. CAN GIVE INFO ABOUT CROSS CHAIN ACTIVITY WITHOUT REVEALING LINKAGE
% data wallet holds data -> how do we collect and store it?
% Browser local storage
%   Key value pairs
%   Simple in-memory database that by default is limited ~5MB
%           modern browsers allow use to ask for more than just that
%   Future: have more sophisticated options for in browser data base (e.g., indexDB)
% Collect
%   Manually imputed data (explicit)
%   Implicit (eg on-chain)
%       -Need user to sign their wallet address
%           LINK MULTIPLE WALLETS IN SAME PLACE
%       -Use indexer to find queries
%       -(future) in-browser event capturing
%   Verified / third party data (future)
Secure storage of data is crucial to allowing people to own their data. Our initial implementation will use a simple in-memory database of key-value pairs using the browser's local storage. In the future, we will use more sophisticated alternatives such as indexDB as the performance needs of the application scales. It is also important to call out the wallet's storage of keys. For the initial version, we are only storing users' public keys and digital signatures in the data wallet, not private keys. Instead, we integrate with existing wallets (e.g., MetaMask) for key management.


\subsubsection{Collection}
The data wallet will also help users own their own data by allowing them to collect the data that they generate. This individualized data mining provides highly accurate data that the protocol allows users to easily monetize. The data the wallet collects has three important properties: explicit/implicit, first/third party, and authenticated/unauthenticated. %Not sure if there's a better term

An important feature to highlight is that multiple addresses can be linked together in the same data wallet, including wallets for separate chains. % should be included but I'm not sure where or how

% Not sure if this should be a paragraph of points
\paragraph{Explicit/Implicit}
The data wallet will either collect data through explicit or implicit means. Explicit data is data directly provided by the user, such as their name, age, or wallet address. In the initial version of the data wallet, the user will manually input this data; however, in the future, the data wallet could feature in-browser event capturing to passively collect this data (e.g., collecting information about what websites a user has visited).


Implicit data is data generated by user action. For example, using a user's wallet address to learn that they swapped specific tokens or have played a web3 game. 


\paragraph{First/Third Party}
Data collected can come from different sources. Specifically, first-party data comes directly from the user, and third-party data comes from someone other than the user. For example, if the user directly inputs their name, their name would be considered first-party data; if the user imports their name from the DMV, that would be third-party data.


\paragraph{Authenticated/Unauthenticated}
Data can be authenticated if its origin and validity can be verified through cryptographic means and unauthenticated if it can't. For example, a wallet address can be authenticated if we receive a signed message from that address. Third-party data can be authenticated if it has a known identity (e.g., the DMV's public key is known, and the data wallet receives data signed by that key). 

% old text
% The data wallet will enable the collection of data in three categories: explicit, implicit, and authenticated. Explicit data is data input by the user, such as their name, age, or wallet address. An important feature to highlight is that multiple addresses can be linked together in the same data wallet, including wallets for separate chains. Implicit data is data generated by user action. For example, on-chain data is generated by a user swapping tokens or playing a web3 game. In the future, the data wallet could feature in-browser event capturing for this purpose, such as collecting information about what websites a user has visited. Authenticated data is data signed and authenticated by a third party. For example, the California DMV can validate that Alice is named Alice and lives in California.


\subsubsection{Localized Processing} % listens to events and processes queries
% What are advantages of this model? 
     % for individuals
     % for businesses
     % future benefits of localized processing in another section% 
% How is this done? 
%   - SDQL querying
%   - Listens for events on chain
%   - Using existing wallets (e.g., metamask) to manage keys. NOT STORING PRIVATE KEYS RN
%   - reference future improvements
% Increased Data Safety
The data wallet is a local application that stores the owner's data securely and processes computations locally. By collecting and securely storing user data locally, the data wallet guarantees data ownership to the user by never sharing it. Because computations are running locally, the owner ensures that only analysis they've given consent to can run on their data. Businesses also benefit from this model as they can leverage data-driven insights without worrying about the liability or infrastructure of storing raw, personal data. While the initial version of localized processing will be more limited, there is a myriad of ways we can modify this approach to add additional data safety and features (see section \ref{section:Future}). 

The wallet will learn what computations to run by listening to on-chain events. These events will be emitted on the Snickerdoodle Avalanche subnet and specify SDQL queries that include the computation instructions. If those data request events are from consent contracts that the user has opted into, the data wallet will process the data following the rules in the associated SDQL query. After the localized processing has been completed, the data wallet will send the insights to the specified endpoint, and the data owner will collect any rewards.


\subsubsection{Snickerdoodle Query Language} % maybe move to contracts 
\label{section:SDQL}
% data app
% SDQL exists to run computations on local data
% Structure
%   - json file
%       - Qualifying
%           mention privacy issue here that's in future -- qualify + reward can it reveal
%       - Rewards
%       - ? who indicated ?
%       - What function to run
%   - maybe discuss CID
%   - example
% why
%   - Decentralized  DAO control of quries
%   - Tamper Resistence
%       - guarentee what we are asking is what is being delivered
%   - CONTENT ADDRESSING
%   - json simplicity
The Snickerdoodle Query Language will allow businesses to share their data requests with individual users in a transparent and auditable fashion. It will be structured as a simple JSON file containing information on the eligibility requirements, rewards, data to be collected, what processing to perform, and where to send processed data (see figure TODO). The allowed functions and syntax of the language will be enforced by the Snickerdoodle DAO and determined by token holders, which will allow for its continued development and the enforcement of user privacy. SDQL queries must be stored on a content-addressable distributed network, such as IPFS, to ensure tamper resistance. 

TODO FIGURE SHOWING QUERY + flesh out sections


\subsubsection{Rewards}
% Rewards
% - See / find rewards being offered 
%       - Websites integrate SDL
%       - Market place of available rewards
% - Manage their rewards
%       - See rewards offered by query
%       - History of accepted rewards
%       - Which wallets rewards should be sent to 

An essential part of the protocol is allowing individuals to get value from their rewards. The data wallet will facilitate this by allowing users to gain rewards for sharing their data. First, the data wallet helps people find rewards that are being offered. If a person goes to a website that's enabled the Snickerdoodle Protocol, they will see a pop-up and can connect their data wallet to check out possible rewards. Additionally, the data wallet will have a section showing available rewards.


Once the user has given consent to share data, the data wallet will also help the user manage their rewards. When they receive a query, they will be able to visualize the rewards offered in that query. The owner will also be able to set which addresses and chains they want to receive rewards. Additionally, the owner will be able to see the history of the rewards they've received and the data exchanged. 

\subsubsection{Authorization and Consent Management}
% How will the wallet know what data to share?
%   - Consent tokens / opt-ins
%   - granular permissions?
%   - opt-outs
% if possible add pictures
Enabling informed consent for data sharing is a core function of the Snickerdoodle Protocol. Consent is accomplished through the use of consent contracts (see section \ref{section:Contracts}). When a user wishes to consent to share data, they make a contract call and are issued a non-transferable consent token. To revoke consent, they can burn that token. The data wallet facilitates this by giving the owner's crypto wallet the appropriate transactions to sign. Once opted in, the data wallet will respond to all eligible queries from that pool unless more granular data permissions have been specified within the wallet. TODO granular permissions
%TODO granular permissions and opting out


For the initial version of the Snickerdoodle Protocol, we are not allowing users to choose what types of queries they can opt in and out of. A user giving consent to a company allows that company to run any query on their data. However, to maintain privacy, we limit queries' interactions with PII. If any queries touch-sensitive data, we will reduce the resolution of the insight, e.g., location data will only reveal the state or country, not the zip code. Additionally, part of the design of the data wallet is that it has the final say on what data is shared. Even if a user consents to a consent contract, a data wallet can theoretically choose not to respond, making it easy for future versions of the wallet to enable more granular privacy controls.

\subsubsection{Distributed Runtime Library}
% Distributed runtime library
%   - Wallet is browser plugin with iFrame
%   - security benefits
%   - upgradability
%       - want to change SDQL push code into iFrame
%       - DAO controls upgrade process (see if patent before publishing paper)

% Wallet is going to be standard browser plugin with iFrame
%.   Describe why
%.   Describe benefits 
% Describe security 
% Mention future section 
The distributed runtime library is a core security feature of the data wallet. The wallet browser plugin will leverage iFrames to run plugins for data processing and wallet integrations. Using iFrames like this will isolate the data wallet's processing from the browser. To ensure the validity of these plugins, we will use the iFrame to enforce the legitimacy of the code by verifying certificates and checksums with the DAO allowlist. In this way, the user has a trustless way to verify their data is being processed and held safely.

\subsubsection{Decentralized certificate authority}
\label{section:DAO}
% checksums and certs enforced by the DAO
%   - accepted logic determined by governance
%   - integrity of the plugins determined by DAO registry
% How are we building this?
%   - DAO is based on compound finance + solidity openzeplin
%   - ERC-721 registry / certificate
%       - Token URI holds the checksum + valid domain name
%   - Registry itself has access control so DAO is only one who can add and remove 
The DAO will maintain an allowlist of authorized plugins determined by governance. This allowlist will ensure that accepted logic is determined by the token holders and enforce the integrity of those plugins at the execution level. The DAO is based on Compound Finance's DAO and built using OpenZeppelin libraries. The certificate registry will be implemented as an ERC-721 contract where the token URI holds the checksum and domain name. The DAO will be the only party authorized to modify this registry. See section \ref{section:TokenDAO} for more details on the DAO.
%reference DAO

\subsection{Consent Contracts} % This kinda feels like current web3 component
\label{section:Contracts}
TODO FIGURE THAT SHOWS HOW SMART CONTRACTS INTERACT
% We want a way for business to signal they want access to data and individuals to signal that they've consented to (some) of the data sharing

% businesses create data pools for themselves
% create queries for data within those pools
% users are compensated for participation
%   - nft rewards attached to queries
%   - doodles (ref: tokenomics)

% Contract features (include diagram)
Another part of the Snickerdoodle Protocol is the creation of consent contracts. Consent contracts allow businesses to create data pools by signaling they want data and give people a way to consent to share data. At a high-level, businesses first create a consent contract via a contract factory. People can then consent to share their data and receive a non-transferable consent token. At this point, the data wallet listens to events emitted from the consent contract. These events contain the CID of an SDQL query. By responding to the query and sharing data, people will receive some reward such as an NFT or a Doodle token (see section \ref{section:tokenomics}).

% dividend instead of reward is an interesting word

% Another part of the Snickerdoodle Protocol is the creation of data pools. Data pools are a way for business to signal they want data and for people to give consent to sharing data. At a high level business first create a data pool by creating a consent contract. People can then consent to sharing their data and receive a consent token. At this point the data wallet listens to events from the consent contract that will send over the CID of a SDQL query. By responding to the query and sharing data, people will receive some reward such as a NFT or a reward (see section \ref{section:tokenomics}).


%TODO diagram

\subsubsection{Contract Factory}
% - consent contract factory -> creates consent contracts
%       - meta transactions
%       - gas optimization / easy protocol updates
%       - detail beacon model here
The consent contract factory exists as an interface for data subscribers to create new consent contracts. The contract factory pattern is a gas-efficient way to make new, upgradeable contracts. The factory contract will be implemented with an upgradable beacon pattern to optimize for gas and make it easier to upgrade. The contracts will also support meta-transactions, allowing companies to pay the gas fees for their user's participation in the network (e.g., minting consent tokens and rewards). The full consent contract factory flow is outlined below: <insert figure>

\paragraph{Factory Pattern}
The factory pattern is a pattern where a smart contract is responsible for creating other contracts. The Snickerdoodle Protocol will use the factory pattern to simplify the deployment of new consent contracts for each subscriber. This simplicity makes it easier for developers to manage, creates a trustless way for subscribers to deploy consent contracts, and improves security by making the contract easier to audit and limiting the scope of each contract. Additionally, the contract factory pattern can reduce the gas cost of deploying new contracts, especially when implemented through proxies and upgradable beacon patterns (see section \ref{section:BeaconPattern}).

\paragraph{Proxies & Upgradable Beacon Patterns}
\label{section:BeaconPattern}
% General description
%       include proxy pattern and point to upgradable beacon proxy
%
% why
%   Optimizes gas of nearly identical consent contracts
%   Upgradability? -- reason we can easily change lots of contracts at the same time
% How we build it
%   openzeplin libraries
%   Upgradability? -- how are we doing it 
%       EIP-1967 specific upgradability for upgradable proxy / storage layer works
%       upgradable beacon
%   pointing to upgradable beacon contract -- discuss in next section

% \paragraph{Beacon}
% maybe  include in pervious section
% contract that points to the implmentation
% updates the proxy contracts
%   reason is it allows for easy upgrades
Consent contracts themselves will be created with proxies and upgradable beacon contracts. This works because the contract factory contract will create new proxy consent contracts. These proxy contracts contain minimal information, only storing their state and a pointer to the upgradable beacon contract. The upgradable beacon contract points to the current implementation of the consent contract. This deployed contract contains all of the functions and stored values of the consent contract, which the proxy consent contracts inherit. If we want to update consent contracts, we can deploy a new consent contract and change the upgradable beacon to point to the new contract. All previously deployed and newly created proxy consent contracts will now inherit the functionality defined in the new contract. Figure TODO shows this flow.

We are using proxies and upgradable beacon patterns to save on gas and improve upgradability. Proxies help reduce gas costs when deploying many identical contracts as each new contract is very small. This pattern also reduces gas costs when upgrading many contracts simultaneously. It also simplifies the process of upgrading, which has DevOps and security benefits.


The upgradability pattern is based on EIP-1967 (CITE) and is implemented through OpenZepplin libraries (CITE).

\paragraph{Meta Transactions}
% General description
%   Enabled for all statechanging txs (anything with gas fees)
%   msgsender gets overridden

% why
%   Easier usability as users don't need to pay money to interact (not strictly necessary)
%   Either SDL or business pays


% how
%   openzeplin
%   Both for creating contracts and creating consent tokens
%   ERC2771ContextUpgradeable

Meta transactions allow someone else to pay for a user's gas fees. Every state-changing transaction (i.e., any transaction with a gas fee) will have meta transactions enabled. We did this to give us the flexibility to have users pay for their own gas or have Snickerdoodle Labs or another business pay. In the initial version, we don't want users to pay for their gas costs as it creates a bad user experience. Users will need to add the native gas token to their Snickerdoodle Wallets, which often involves buying a new token, bridging that token, and paying gas along the way. Also, people don't like spending money on gas costs (do I need to cite?). 

Meta transactions follow the ERC2771 standard and are implemented using OpenZepplin's libraries (CITE).


\subsubsection{Consent Contract}
\label{section:ConsentContract}
% - issues consent tokens for opt-in
% - elgibility logic for opt-ins
% - emits events when query is published with SDQL CID
The consent contract represents a single data pool and signals the desire to subscribe to data and facilitates the exchange of insights. These contracts issue consent tokens for opt-in and allow the token burning for opt-outs. The contracts can define eligibility and other rules around user opt-ins. Consent contracts are also responsible for emitting data requests for the wallet listeners.

% reference previous section about implementation


\subsubsection{Consent Tokens}
% represent opt-ins on the user side
% essentially cookies in the snickerdoodle model
% can offer rewards to incentivize opt-in (not made yet...) (ref:tokenomics)
% can be burned to opt-out
% non-transferable
Consent tokens are non-transferable NFTs that conform to the ERC-721 standard. A user owning a token represents that user's consent to participate in the data pool with which the token is associated. Conversely, burning this token represents a repudiation of that consent. Because consent can't be transferred, these tokens are non-transferable.

% include the reason they are non-transferable

Data subscribers can know how many people have consented to share data with them and can respond to their queries. Knowing how many people have consented to a particular subscriber allows the Snickerdoodle Protocol to have a fee structure where subscribers who access more data pay more. The protocol will do this by charging a fee in Doodle tokens proportional to the number of consent tokens to emit the event. For more information about the fee structure and the token, see section \ref{section:tokenomics}.

\subsubsection{Doodle Token}
\label{section:DoodleToken}
% general description
% ERC-20 
% why 
%   token for governance and fees
% how 
%      some kind of wrapping for subnet
%       3 implementations bc we plan on subnet (2 on subnet)
%           native DOODLE (not ERC-20) -- wrap with DAO contract
%           WDOODLE (ERC-20) so you can vote in DAO 
%           ----- not subnet below -----
%           vanilla DOODLE (ERC-20) linked on C-Chain+others for other bridging ERC-20 infrastructure 
%       If on a different EVM network:
%              Just DOODLE (ERC-20)
%           vanilla DOODLE (ERC-20) linked for bridging ERC-20 infrastructure 
%   ERC-20 + openzepplin
The token that powers the protocol will be the Doodle Token. This token will be used for paying Snickerdoodle Subnet gas fees, paying Snickerdoodle Protocol fees, and voting in the Snickerdoodle DAO. 

The Doodle Token will come in several forms to support these use cases. First, it is a native non-ERC-20 token of the Snickerdoodle subnet. This native token will be used to pay both gas and protocol fees. The second form will be an ERC-20 token, Wrapped Doodle (WDOODLE), representing votes in the DAO. The DAO itself will have a contract that can convert DOODLE and WDOODLE back and forth. Lastly, there will be vanilla ERC-20 implementations of the DOODLE on other EVM chains to enable bridging so the DOODLE can be used with other standard ERC-20 infrastructure. We will use openZepplin Libraries to implement these contracts. 


\subsubsection{DAO}
\label{section:ImplementationDAO}
% general description
% why
%   Need decentralized way to control allowed queries, certificate authorities, allow/ban lists for known good/bad actors
% how
%   openzeplin curve finance
The DAO is the decentralized governance mechanism of the Snickerdoodle Protocol. The DAO creates a decentralized way to manage allowed queries, acts as a certificate authority for the code distribution, and controls an allow/ban list of good/bad actors. The DAO will be modeled on Curve Finance's DAO (CITE) and will be implemented with OpenZepplin libraries (CITE). 

\subsubsection{Queries}
% SDQL queries (add visuals)
% need to be content-addressed (IPFS, Arweave, etc)
% will be published with the request for verifiability
Queries are responsible for specifying the who, what, and why of data processing. They allow data subscribers to define who they want data from, what type of computation to run, where to send the insight, and the reward for sending the insight. These queries must be content-addressed, so anyone can access the query and verify its validity. For the initial version of the Snickerdoodle Protocol, we will be publishing these queries on IPFS. In the future, the protocol will support queries hosted through other means.

TODO add info only allow queries if there are 30+ consent tokens given out

\subsubsection{Network \& Fees}
% general description
%   One an avax subnet
%   ARE WE SUBNET OR ON ANOTHER NETWORK
% why
%   Want scalability and lower fees
%   EVM
% how
%   Still 

% fees
%   fee in native currency for any statechanging action we take (gas)
%       (creating contract, consent tokens, emitting events)
%   extra fee in DOODLEs for accessing insights
%   rewards
The Snickerdoodle Protocol will be deployed on the Snickerdoodle Network, which will be an EVM-compatible Avalanche Subnet. Our requirements for the network were we wanted a scalable network with low fees and EVM compatibility. Making an Avalanche Subnet fit these requirements (IDK more and IDK the how).

As mentioned in \ref{section:DoodleToken}, the native token of this subnet will be the Doodle and will be used for paying fees. Within the context of the protocol and managing data, gas fees will be paid for creating consent contracts, creating consent tokens, and emitting events (i.e., SDQL queries). Data subscribers will pay additional protocol fees to emit an SDQL query to access insights. This protocol fee will be proportional to the amount of data being accessed and can be calculated using the number of consent tokens tied to a specific contract.

\subsubsection{Rewards}
% Initial version will be trust centric
%   Get query give data, trust get reward, trust get valid data
% Atomic swaps / escrow something to look at
%   Simple way would be DOODLEs as only reward that this works with
%   Future look into ZK transfers
After sharing insights, the protocol will facilitate the transfer of rewards to the data owner. The owner can choose which wallet to send rewards to, not just the data wallet. In the initial version of the protocol, the insights/rewards transfer process will be trust-centric. When an owner responds to a query, they must trust that the business will send them the reward later. When subscribers send a reward, they need to trust that the user is sending over authentic insights. In future versions, we will enable atomic reward swaps. Atomic swaps allow insights to be shared at the same instance that a reward is received. We could easily add this functionality if the rewards are in DOODLE tokens, and zero-knowledge proofs could enable atomic swaps with different assets across chains (see \ref{section:AtomicSwaps}).

\subsubsection{Privacy}
% This is how users signal consent
% Trade-off of public knowledge with utility
%   Privacy of everyone knowing who's giving consent to Shrapnel
%   Everyone knows what queries Shrapnel is interested in
%       Potential loss of competitive edge + buisisness privacy
%           - Reason is Philosophical argument (transparency, control, risk reduction)
%           - Need future section about creating private query cohorts
% Reasons why we are ok with tradeoffs made for now
A user receiving a consent token is how users signal their consent and a business emitting an event is how they signal they are interested in a particular insight. This transparency in knowledge does come with some privacy tradeoffs:
\begin{itemize}
  \item Everyone knows who is giving consent to a business
  \item Everyone knows what queries a business has run
\end{itemize}

These privacy trade-offs are acceptable for the initial version of the protocol. Our goals are to make sure users are able to give informed consent to the use of their data, data isn't revealed if this consent isn't given, insights rather than raw data are revealed if consent is given, and businesses will pay more to send queries to more people. The use of public consent tokens allows us to achieve these goals with an acceptable amount of privacy. It also gives the initial version of the protocol an interoperability bonus by allowing the consent tokens to integrate with existing web3 solutions. By making consent a public token that conforms to the ERC-721 standard, people can see what they've consented to through any application that shows owned NFTs. In a future version, we could create ways to hide this information. For example, an anonymized consent token could be minted while the data wallet listens and responds to events. %TODO maybe reference future


It's also worth pointing out that this decision doesn't reduce the privacy of individuals too much more than if we didn't use public consent tokens. Accepting rewards linked to a business reveals that consent has been given to that business (i.e., Alice having a Bob's Business NFT means everyone knows Alice consented to provide information to Bob's Business). Additionally, there will be a limited number of subscribers for the initial release, all of which will be web3 companies. This limitation reduces the information revealed by owning a token. Holding a consent token for a particular business indicates interaction with the business and the willingness to adopt a web3 product for a reward. The former is already public knowledge by looking at the blockchain's history, and the latter is a common trait of web3 users.


We are also ok reducing businesses' privacy by making what queries are run public. We feel that more transparency in what information businesses are interested in gives people more insight into whether they want to share their data and reduces the societal risk of companies running complex insights that hurt the commonwealth. If we need to add more privacy to businesses, we could add it in future versions by creating private query cohorts. See section \ref{section:Future} for a discussion on future changes to the protocol that can increase privacy.

\subsection{Data Ingestion Provider} % name for role?
% How we allow data subscribing and apps
% execution role for data applications (SDQL queries)
% can be run by any party supporting protocol interface
%   - SDL will offer SaaS solution
% will be the endpoint of the SDQL query for data aggregation
% highlight that anyone can make their own
Data ingestion is the final stage of the insight aggregation process that allows subscribers to access the insights generated at the data wallet layer. This section will detail how the subscriber can see processed data from the user's data wallet. Any entity can assume the role of data ingestion service as long as they follow the protocol and are accepted by the DAO. Snickerdoodle Labs will offer the first data ingestion provider, which will be tied to a SaaS product. 

\subsubsection{Ingestion}
The final step of the protocol is for data wallets to send insights from their queries to an endpoint. This endpoint is the ingestion provider and is specified in the SDQL query. Ingestion is an integral part of the protocol as it allows businesses to see the insights they have paid for.



Once the ingestion provider receives the insight, it must store the insights events and manage access. They can also provide any additional services they want on top of this, such as providing a dashboard to view insights. There is no way to mathematically guarantee any ingestion provider is behaving honestly or doesn't have any bugs. The lack of a strong guarantee means that actors in the protocol have to put some trust in ingestion providers. The Snickerdoodle Protocol reduces this problem by only allowing ingestion services that the DAO has approved. Any provider has to be trustworthy enough for the DAO to vote them in; if a provider is revealed to be a bad or incompetent actor, they can be voted out. For a deeper discussion of data safety concerns with ingestion providers, see section \ref{section:IngestionDataSafety}.

\subsubsection{Data Safety}
\label{section:IngestionDataSafety}
% Lots of trust into aggreator
% lack of anonymization
% data security
% leak risk
% legal compliance
The initial version of the protocol requires trust in the Insight Platform to maintain data safety. As discussed above, there are no guarantees that the ingestion provider acts honestly. The ingestion provider can see, delete, and sell any insights they receive. Additionally, because the initial version of the wallet only implements simple anonymization techniques, ingestion providers could identify who shared the insight and reconstruct the raw data. 


To reduce these privacy risks, the initial version of the protocol will only allow queries that don't reveal PII, and future versions will add anonymization techniques to insight computations (see section \ref{section:Future}). To reduce the risk of malicious ingestion services, the Snickerdoodle DAO will maintain an allowlist of trusted actors. Additionally, Snickerdoodle Labs will be providing an ingestion service. The Snickerdoodle Ingisht Service is a SaaS product that will enforce data safety and has a massive financial and legal incentive to behave honestly.

\subsubsection{SDL Insight Platform}
\label{section:InsightService}
% in-depth on SDL SaaS product
%   - data aggregation
%   - managed querying
%   - pool management
%   - pinning service / content addressing
% Dashboard / analytics
%   Maybe plugin dashboards
The Snickerdoodle Insight Platform will be our SaaS product offering for interaction with the Snickerdoodle Protocol. The Insight Platform will provide an ingestion service and help manage interaction with the rest of the protocol. This includes support for creating and managing queries, consent contracts, rewards, and website integrations. The Insight Platform will also provide an analytics dashboard to help businesses see their insights.